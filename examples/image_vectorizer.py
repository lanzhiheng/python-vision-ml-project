#!/usr/bin/env python3
"""
å›¾åƒå‘é‡åŒ–å·¥å…· - å¤šæ¨¡å‹é›†æˆ & CPU/GPUæ€§èƒ½å¯¹æ¯”

è¿™ä¸ªè„šæœ¬ä½¿ç”¨å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹é›†æˆå¯¹å›¾åƒè¿›è¡Œå‘é‡åŒ–ï¼š
- ResNet-152: æ·±åº¦æ®‹å·®ç½‘ç»œ (2048ç»´)
- EfficientNet-B7: é«˜æ•ˆç½‘ç»œ (2560ç»´)
- Vision Transformer (ViT-L/16): ç°ä»£Transformeræ¶æ„ (1024ç»´)

ç‰¹ç‚¹:
- æ˜¾è‘—å¢åŠ è®¡ç®—å¤æ‚åº¦ï¼Œä¾¿äºæ¸…æ™°å¯¹æ¯”CPU/GPUæ€§èƒ½å·®å¼‚
- æœ€ç»ˆç‰¹å¾ç»´åº¦: 5632ç»´ (æ‰€æœ‰æ¨¡å‹æ‹¼æ¥)
- æ”¯æŒåŠ¨æ€æ¨¡å‹é€‰æ‹©å’Œæ‰¹é‡å¤„ç†

ä½¿ç”¨æ–¹æ³•:
    python examples/image_vectorizer.py --image_path /path/to/image.jpg --device cpu
    python examples/image_vectorizer.py --image_path /path/to/images/ --device gpu --batch_size 8
    python examples/image_vectorizer.py --models resnet152,vit_l_16 --image_path image.jpg
"""

import torch
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import os
import argparse
import time
import numpy as np
from typing import Dict, Union, Optional
import sys

# CLIPæ¨¡å‹å¯¼å…¥ï¼ˆå»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¼ºåˆ¶ä¾èµ–ï¼‰
try:
    import clip
    OPENAI_CLIP_AVAILABLE = True
except ImportError:
    OPENAI_CLIP_AVAILABLE = False
    print("âš ï¸ OpenAI CLIP ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install clip-by-openai")

try:
    import open_clip
    OPEN_CLIP_AVAILABLE = True
except ImportError:
    OPEN_CLIP_AVAILABLE = False
    print("âš ï¸ OpenCLIP ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install open-clip-torch")

class ImageVectorizer:
    """
    å›¾åƒå‘é‡åŒ–å™¨ï¼Œä½¿ç”¨å¤šæ¨¡å‹é›†æˆæå–å›¾åƒç‰¹å¾å‘é‡ã€‚
    
    ç‰¹ç‚¹ï¼š
    - æ”¯æŒæ¨¡å‹é›†æˆ (ResNet-152, EfficientNet-B7, ViT-L/16)
    - æ˜¾è‘—å¢åŠ è®¡ç®—å¤æ‚åº¦ï¼Œä¾¿äºå¯¹æ¯”CPU/GPUæ€§èƒ½
    - æ”¯æŒå•å¼ å›¾ç‰‡å’Œæ‰¹é‡å¤„ç†
    - è‡ªåŠ¨è®¾å¤‡æ£€æµ‹å’Œåˆ‡æ¢
    - è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡
    - åŠ¨æ€æ¨¡å‹é€‰æ‹©
    """

    # æ”¯æŒçš„æ¨¡å‹é…ç½®
    SUPPORTED_MODELS = {
        # åŸæœ‰çš„torchvisionæ¨¡å‹
        'resnet152': {
            'input_size': 224,
            'expected_feature_dim': 2048,
            'description': 'ResNet-152 æ·±åº¦æ®‹å·®ç½‘ç»œ',
            'model_type': 'torchvision'
        },
        'resnet101': {
            'input_size': 224,
            'expected_feature_dim': 2048,
            'description': 'ResNet-101 æ·±åº¦æ®‹å·®ç½‘ç»œ',
            'model_type': 'torchvision'
        },
        'resnet50': {
            'input_size': 224,
            'expected_feature_dim': 2048,
            'description': 'ResNet-50 æ·±åº¦æ®‹å·®ç½‘ç»œ',
            'model_type': 'torchvision'
        },
        'efficientnet_b7': {
            'input_size': 600,
            'expected_feature_dim': 2560,
            'description': 'EfficientNet-B7 é«˜æ•ˆç½‘ç»œ',
            'model_type': 'torchvision'
        },
        'vit_l_16': {
            'input_size': 224, 
            'expected_feature_dim': 1024,
            'description': 'Vision Transformer Large',
            'model_type': 'torchvision'
        },
        
        # OpenAI CLIP æ¨¡å‹
        'openai_clip_vit_b32': {
            'input_size': 224,
            'expected_feature_dim': 512,
            'description': 'OpenAI CLIP ViT-B/32 (512ç»´)',
            'model_type': 'openai_clip',
            'model_name': 'ViT-B/32'
        },
        'openai_clip_vit_b16': {
            'input_size': 224,
            'expected_feature_dim': 512,
            'description': 'OpenAI CLIP ViT-B/16 (512ç»´)',
            'model_type': 'openai_clip',
            'model_name': 'ViT-B/16'
        },
        'openai_clip_vit_l14': {
            'input_size': 224,
            'expected_feature_dim': 768,
            'description': 'OpenAI CLIP ViT-L/14 (768ç»´)',
            'model_type': 'openai_clip',
            'model_name': 'ViT-L/14'
        },
        'openai_clip_rn50': {
            'input_size': 224,
            'expected_feature_dim': 1024,
            'description': 'OpenAI CLIP ResNet-50 (1024ç»´)',
            'model_type': 'openai_clip',
            'model_name': 'RN50'
        },
        'openai_clip_rn101': {
            'input_size': 224,
            'expected_feature_dim': 512,
            'description': 'OpenAI CLIP ResNet-101 (512ç»´)',
            'model_type': 'openai_clip',
            'model_name': 'RN101'
        },
        
        # OpenCLIP æ¨¡å‹ (å¢å¼ºç‰ˆæœ¬)
        'open_clip_vit_b32_openai': {
            'input_size': 224,
            'expected_feature_dim': 512,
            'description': 'OpenCLIP ViT-B/32 OpenAI Dataset (512ç»´)',
            'model_type': 'open_clip',
            'model_name': 'ViT-B-32',
            'pretrained': 'openai'
        },
        'open_clip_vit_b32_laion400m': {
            'input_size': 224,
            'expected_feature_dim': 512,
            'description': 'OpenCLIP ViT-B/32 LAION-400M (512ç»´)',
            'model_type': 'open_clip',
            'model_name': 'ViT-B-32',
            'pretrained': 'laion400m_e32'
        },
        'open_clip_vit_b16_laion400m': {
            'input_size': 224,
            'expected_feature_dim': 512,
            'description': 'OpenCLIP ViT-B/16 LAION-400M (512ç»´)',
            'model_type': 'open_clip',
            'model_name': 'ViT-B-16',
            'pretrained': 'laion400m_e32'
        },
        'open_clip_vit_l14_laion2b': {
            'input_size': 224,
            'expected_feature_dim': 768,
            'description': 'OpenCLIP ViT-L/14 LAION-2B (768ç»´)',
            'model_type': 'open_clip',
            'model_name': 'ViT-L-14',
            'pretrained': 'laion2b_s32b_b82k'
        },
        'open_clip_vit_h14_laion2b': {
            'input_size': 224,
            'expected_feature_dim': 1024,
            'description': 'OpenCLIP ViT-H/14 LAION-2B (1024ç»´)',
            'model_type': 'open_clip',
            'model_name': 'ViT-H-14',
            'pretrained': 'laion2b_s32b_b79k'
        },
        'open_clip_convnext_base': {
            'input_size': 256,
            'expected_feature_dim': 512,
            'description': 'OpenCLIP ConvNeXt-Base LAION-400M (512ç»´)',
            'model_type': 'open_clip',
            'model_name': 'convnext_base',
            'pretrained': 'laion400m_s13b_b51k'
        },
        'open_clip_convnext_large': {
            'input_size': 320,
            'expected_feature_dim': 768,
            'description': 'OpenCLIP ConvNeXt-Large LAION-400M (768ç»´)',
            'model_type': 'open_clip',
            'model_name': 'convnext_large_d_320',
            'pretrained': 'laion2b_s29b_b131k_ft_soup'
        }
    }

    def __init__(self, device: str = 'cpu', model_names: list = None):
        """
        åˆå§‹åŒ–å‘é‡åŒ–å™¨ã€‚

        Args:
            device (str): ä½¿ç”¨çš„è®¾å¤‡ ('cpu' æˆ– 'gpu')
            model_names (list): è¦ä½¿ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼Œé»˜è®¤ä½¿ç”¨æ‰€æœ‰æ¨¡å‹
        """
        self.device_name = device
        self.model_names = model_names if model_names else ['resnet152']  # é»˜è®¤ä½¿ç”¨å•ä¸ªæ¨¡å‹è¿›è¡Œæµ‹è¯•
        self.models = {}
        self.feature_extractors = {}
        self.preprocessors = {}
        self.device = None
        self.total_feature_dim = 0
        self._setup_models()

    def _setup_models(self):
        """
        åŠ è½½å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹å¹¶è®¾ç½®é¢„å¤„ç†ç®¡é“ã€‚
        """
        print(f"æ­£åœ¨è®¾ç½®æ¨¡å‹é›†æˆ {self.model_names} åœ¨è®¾å¤‡: '{self.device_name}'")
        
        # 1. è®¾ç½®è®¡ç®—è®¾å¤‡
        if self.device_name in ('gpu', 'cuda') and torch.cuda.is_available():
            self.device = torch.device("cuda")
            print(f"âœ“ CUDA GPUå·²é€‰æ‹©ã€‚ä½¿ç”¨è®¾å¤‡: {torch.cuda.get_device_name(0)}")
            print(f"  GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        elif self.device_name == 'mps' and torch.backends.mps.is_available():
            self.device = torch.device("mps")
            print("âœ“ Apple Silicon GPU (MPS) å·²é€‰æ‹©")
        else:
            if self.device_name in ('gpu', 'cuda'):
                print("âš ï¸ CUDA GPUä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
            elif self.device_name == 'mps':
                print("âš ï¸ Apple Silicon GPU (MPS) ä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
            self.device = torch.device("cpu")
            print("âœ“ ä½¿ç”¨è®¾å¤‡: CPU")

        # 2. é€ä¸€åŠ è½½æ‰€æœ‰æ¨¡å‹
        print("æ­£åœ¨åŠ è½½å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹...")
        print("=" * 50)
        
        successful_models = []
        self.total_feature_dim = 0
        
        for model_name in self.model_names:
            if model_name not in self.SUPPORTED_MODELS:
                print(f"âš ï¸ ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}ï¼Œè·³è¿‡")
                continue
                
            try:
                model_config = self.SUPPORTED_MODELS[model_name]
                model_type = model_config.get('model_type', 'torchvision')
                print(f"æ­£åœ¨åŠ è½½: {model_config['description']} ({model_name})")
                
                model = None
                preprocessor = None
                feature_extractor = None
                
                # æ ¹æ®æ¨¡å‹ç±»å‹åŠ è½½ä¸åŒçš„æ¨¡å‹
                if model_type == 'torchvision':
                    # åŸæœ‰çš„torchvisionæ¨¡å‹åŠ è½½
                    if model_name == 'resnet152':
                        model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V2)
                    elif model_name == 'resnet101':
                        model = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V2)
                    elif model_name == 'resnet50':
                        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)
                    elif model_name == 'efficientnet_b7':
                        try:
                            model = models.efficientnet_b7(weights=models.EfficientNet_B7_Weights.IMAGENET1K_V1)
                        except AttributeError:
                            print(f"  EfficientNet-B7 ä¸å¯ç”¨ï¼Œæ›¿æ¢ä¸º ResNet-101")
                            model = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V2)
                            model_name = 'resnet101'  # æ›´æ–°æ¨¡å‹åç§°
                    elif model_name == 'vit_l_16':
                        try:
                            model = models.vit_l_16(weights=models.ViT_L_16_Weights.IMAGENET1K_V1)
                        except AttributeError:
                            print(f"  Vision Transformer ä¸å¯ç”¨ï¼Œæ›¿æ¢ä¸º ResNet-50")
                            model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)
                            model_name = 'resnet50'  # æ›´æ–°æ¨¡å‹åç§°
                    else:
                        raise ValueError(f"Unsupported torchvision model: {model_name}")
                    
                elif model_type == 'openai_clip':
                    # OpenAI CLIPæ¨¡å‹åŠ è½½
                    if not OPENAI_CLIP_AVAILABLE:
                        print(f"  è·³è¿‡ {model_name}: OpenAI CLIP ä¸å¯ç”¨")
                        continue
                    
                    clip_model_name = model_config['model_name']
                    print(f"  æ­£åœ¨ä¸‹è½½ OpenAI CLIP æ¨¡å‹: {clip_model_name}...")
                    model, preprocessor = clip.load(clip_model_name, device=self.device)
                    feature_extractor = model  # CLIPæ¨¡å‹æœ¬èº«å°±æ˜¯ç‰¹å¾æå–å™¨
                    
                elif model_type == 'open_clip':
                    # OpenCLIPæ¨¡å‹åŠ è½½
                    if not OPEN_CLIP_AVAILABLE:
                        print(f"  è·³è¿‡ {model_name}: OpenCLIP ä¸å¯ç”¨")
                        continue
                    
                    clip_model_name = model_config['model_name']
                    pretrained = model_config['pretrained']
                    print(f"  æ­£åœ¨ä¸‹è½½ OpenCLIP æ¨¡å‹: {clip_model_name} ({pretrained})...")
                    model, _, preprocessor = open_clip.create_model_and_transforms(
                        clip_model_name, pretrained=pretrained, device=self.device
                    )
                    feature_extractor = model  # OpenCLIPæ¨¡å‹æœ¬èº«å°±æ˜¯ç‰¹å¾æå–å™¨
                else:
                    raise ValueError(f"Unsupported model type: {model_type}")
                
                # å¯¹äºtorchvisionæ¨¡å‹ï¼Œéœ€è¦ç§»é™¤æœ€åçš„åˆ†ç±»å±‚è·å–ç‰¹å¾æå–å™¨
                if model_type == 'torchvision':
                    if 'vit' in model_name:
                        # Vision Transformer çš„ç‰¹æ®Šå¤„ç† - ç§»é™¤headséƒ¨åˆ†
                        feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])
                    else:
                        # ResNet å’Œ EfficientNet çš„å¤„ç† - ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
                        feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])
                    
                    feature_extractor.eval()
                    
                    # ç§»åŠ¨åˆ°è®¾å¤‡
                    print(f"  æ­£åœ¨å°† {model_name} ç§»åŠ¨åˆ°è®¡ç®—è®¾å¤‡...")
                    start_time = time.time()
                    feature_extractor.to(self.device)
                    end_time = time.time()
                    
                    # è®¾ç½®é¢„å¤„ç†ç®¡é“
                    input_size = model_config['input_size']
                    preprocessor = transforms.Compose([
                        transforms.Resize(int(input_size * 1.14)),  # ç•¥å¤§äºç›®æ ‡å°ºå¯¸
                        transforms.CenterCrop(input_size),
                        transforms.ToTensor(),
                        transforms.Normalize(
                            mean=[0.485, 0.456, 0.406], 
                            std=[0.229, 0.224, 0.225]
                        ),
                    ])
                    
                elif model_type in ['openai_clip', 'open_clip']:
                    # CLIPæ¨¡å‹å·²ç»åœ¨åŠ è½½æ—¶ç§»åŠ¨åˆ°è®¾å¤‡äº†
                    print(f"  {model_name} å·²åŠ è½½åˆ°è®¾å¤‡: {self.device}")
                    start_time = time.time()
                    end_time = time.time()  # CLIPæ¨¡å‹åœ¨åŠ è½½æ—¶å·²ç»ç§»åŠ¨åˆ°è®¾å¤‡
                    # preprocessor å·²ç»åœ¨åŠ è½½æ—¶è·å¾—
                    
                # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
                if hasattr(feature_extractor, 'eval'):
                    feature_extractor.eval()
                
                # è·å–å®é™…ç‰¹å¾ç»´åº¦
                with torch.no_grad():
                    input_size = model_config['input_size']
                    dummy_input = torch.randn(1, 3, input_size, input_size).to(self.device)
                    
                    if model_type == 'torchvision':
                        dummy_output = feature_extractor(dummy_input)
                        actual_feature_dim = dummy_output.squeeze().numel()
                    elif model_type in ['openai_clip', 'open_clip']:
                        # CLIPæ¨¡å‹ä½¿ç”¨encode_imageæ–¹æ³•
                        dummy_output = feature_extractor.encode_image(dummy_input)
                        actual_feature_dim = dummy_output.squeeze().numel()
                    else:
                        actual_feature_dim = model_config['expected_feature_dim']  # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
                
                # ä¿å­˜æ¨¡å‹ç»„ä»¶
                self.models[model_name] = model
                self.feature_extractors[model_name] = feature_extractor
                self.preprocessors[model_name] = preprocessor
                self.total_feature_dim += actual_feature_dim
                
                successful_models.append(model_name)
                print(f"âœ“ {model_name} åŠ è½½å®Œæˆ (ç»´åº¦: {actual_feature_dim}, è€—æ—¶: {end_time - start_time:.2f}ç§’)")
                
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {model_name} åŠ è½½å¤±è´¥: {e}")
                print(f"  å°†ç»§ç»­åŠ è½½å…¶ä»–æ¨¡å‹...")
                continue
        
        if not successful_models:
            raise RuntimeError("æ‰€æœ‰æ¨¡å‹åŠ è½½éƒ½å¤±è´¥ï¼")
        
        self.model_names = successful_models
        print("=" * 50)
        print(f"âœ“ æ¨¡å‹é›†æˆåˆå§‹åŒ–å®Œæˆï¼")
        print(f"  åŠ è½½æˆåŠŸçš„æ¨¡å‹: {self.model_names}")
        print(f"  æ€»ç‰¹å¾ç»´åº¦: {self.total_feature_dim}")
        print(f"  è®¡ç®—å¤æ‚åº¦å€æ•°: {len(self.model_names)}x")
        print("=" * 50)

    def vectorize_image(self, image_path: str) -> np.ndarray:
        """
        å¯¹å•å¼ å›¾åƒè¿›è¡Œå¤šæ¨¡å‹å‘é‡åŒ–ã€‚

        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„

        Returns:
            np.ndarray: æ‹¼æ¥åçš„é«˜ç»´ç‰¹å¾å‘é‡
        """
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶æœªæ‰¾åˆ°: {image_path}")

        try:
            # åŠ è½½åŸå§‹å›¾åƒ
            img = Image.open(image_path).convert('RGB')
            
            # å­˜å‚¨æ‰€æœ‰æ¨¡å‹çš„ç‰¹å¾å‘é‡
            feature_vectors = []
            
            # å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œæ¨ç†
            for model_name in self.model_names:
                # ä½¿ç”¨å¯¹åº”æ¨¡å‹çš„é¢„å¤„ç†å™¨
                preprocessor = self.preprocessors[model_name]
                feature_extractor = self.feature_extractors[model_name]
                model_config = self.SUPPORTED_MODELS[model_name]
                model_type = model_config.get('model_type', 'torchvision')
                
                # é¢„å¤„ç†å›¾åƒ
                if model_type in ['openai_clip', 'open_clip']:
                    # CLIPæ¨¡å‹çš„é¢„å¤„ç†å™¨è¿”å›çš„æ˜¯tensor
                    if hasattr(preprocessor, '__call__'):
                        img_tensor = preprocessor(img).unsqueeze(0).to(self.device)
                    else:
                        # å¦‚æœæ˜¯ä¼ ç»Ÿçš„transforms
                        img_tensor = preprocessor(img)
                        batch_tensor = torch.unsqueeze(img_tensor, 0).to(self.device)
                        img_tensor = batch_tensor
                else:
                    # torchvisionæ¨¡å‹çš„é¢„å¤„ç†
                    img_tensor = preprocessor(img)
                    img_tensor = torch.unsqueeze(img_tensor, 0).to(self.device)

                # æå–ç‰¹å¾
                with torch.no_grad():
                    if model_type == 'torchvision':
                        features = feature_extractor(img_tensor)
                    elif model_type in ['openai_clip', 'open_clip']:
                        # CLIPæ¨¡å‹ä½¿ç”¨encode_imageæ–¹æ³•
                        features = feature_extractor.encode_image(img_tensor)
                    else:
                        features = feature_extractor(img_tensor)
                
                # å±•å¹³ä¸ºä¸€ç»´å‘é‡å¹¶æ·»åŠ åˆ°åˆ—è¡¨
                vector = features.squeeze().cpu().numpy().flatten()
                feature_vectors.append(vector)
            
            # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾å‘é‡
            concatenated_vector = np.concatenate(feature_vectors)
            return concatenated_vector
            
        except Exception as e:
            print(f"âŒ å¤„ç†å›¾åƒæ—¶å‡ºé”™ {image_path}: {e}")
            raise

    def vectorize_directory(self, dir_path: str, batch_size: int = 8) -> Optional[Dict]:
        """
        æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰å›¾åƒã€‚

        Args:
            dir_path (str): å›¾åƒç›®å½•è·¯å¾„
            batch_size (int): æ‰¹å¤„ç†å¤§å°

        Returns:
            dict: åŒ…å«å‘é‡å’Œæ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
        """
        if not os.path.isdir(dir_path):
            raise NotADirectoryError(f"ç›®å½•æœªæ‰¾åˆ°: {dir_path}")

        # æŸ¥æ‰¾æ‰€æœ‰å›¾åƒæ–‡ä»¶
        supported_formats = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.webp')
        image_files = []
        
        for file in os.listdir(dir_path):
            if file.lower().endswith(supported_formats):
                image_files.append(os.path.join(dir_path, file))
        
        if not image_files:
            print("âŒ ç›®å½•ä¸­æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
            return None

        print(f"ğŸ“ æ‰¾åˆ° {len(image_files)} å¼ å›¾åƒ")
        print(f"ğŸ”§ æ‰¹å¤„ç†å¤§å°: {batch_size}")
        print("-" * 30)

        all_vectors = {}
        total_time = 0
        processed_count = 0
        failed_count = 0
        
        # æ‰¹é‡å¤„ç†
        for i in range(0, len(image_files), batch_size):
            batch_paths = image_files[i:i+batch_size]
            batch_tensors = []
            valid_paths = []
            
            # ç›´æ¥è®°å½•æœ‰æ•ˆè·¯å¾„ï¼Œé¢„å¤„ç†å°†åœ¨æ¯ä¸ªæ¨¡å‹ä¸­è¿›è¡Œ
            for path in batch_paths:
                try:
                    # ç®€å•éªŒè¯å›¾åƒæ–‡ä»¶æ˜¯å¦å¯è¯»
                    img = Image.open(path).convert('RGB')
                    valid_paths.append(path)
                except Exception as e:
                    print(f"âš ï¸ è·³è¿‡æŸåçš„å›¾åƒ: {os.path.basename(path)} ({e})")
                    failed_count += 1
                    continue
            
            if not valid_paths:
                continue
            
            # è®°å½•æ¨ç†æ—¶é—´å¹¶è¿›è¡Œå¤šæ¨¡å‹æ¨ç†
            start_time = time.time()
            
            # å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œæ‰¹é‡æ¨ç†
            all_model_features = {}
            for model_name in self.model_names:
                # é‡æ–°é¢„å¤„ç†å½“å‰æ‰¹æ¬¡ï¼ˆé’ˆå¯¹ä¸åŒæ¨¡å‹çš„è¾“å…¥å°ºå¯¸ï¼‰
                preprocessor = self.preprocessors[model_name]
                feature_extractor = self.feature_extractors[model_name]
                model_config = self.SUPPORTED_MODELS[model_name]
                model_type = model_config.get('model_type', 'torchvision')
                
                model_batch_tensors = []
                for path in valid_paths:
                    img = Image.open(path).convert('RGB')
                    if model_type in ['openai_clip', 'open_clip']:
                        # CLIPæ¨¡å‹çš„é¢„å¤„ç†
                        if hasattr(preprocessor, '__call__'):
                            img_tensor = preprocessor(img)
                        else:
                            img_tensor = preprocessor(img)
                    else:
                        # torchvisionæ¨¡å‹çš„é¢„å¤„ç†
                        img_tensor = preprocessor(img)
                    model_batch_tensors.append(img_tensor)
                
                if model_batch_tensors:
                    model_batch_tensor = torch.stack(model_batch_tensors).to(self.device)
                    
                    with torch.no_grad():
                        if model_type == 'torchvision':
                            features = feature_extractor(model_batch_tensor)
                        elif model_type in ['openai_clip', 'open_clip']:
                            # CLIPæ¨¡å‹ä½¿ç”¨encode_imageæ–¹æ³•
                            features = feature_extractor.encode_image(model_batch_tensor)
                        else:
                            features = feature_extractor(model_batch_tensor)
                    
                    all_model_features[model_name] = features.squeeze().cpu().numpy()
            
            # åŒæ­¥GPUæ“ä½œ (å¦‚æœä½¿ç”¨GPU)
            if self.device.type == 'cuda':
                torch.cuda.synchronize()
            
            end_time = time.time()
            batch_time = end_time - start_time
            total_time += batch_time
            
            # æ‹¼æ¥æ‰€æœ‰æ¨¡å‹çš„ç‰¹å¾å‘é‡
            batch_size = len(valid_paths)
            for j, path in enumerate(valid_paths):
                filename = os.path.basename(path)
                
                # æ”¶é›†å½“å‰æ ·æœ¬çš„æ‰€æœ‰æ¨¡å‹ç‰¹å¾
                sample_features = []
                for model_name in self.model_names:
                    model_features = all_model_features[model_name]
                    
                    # å¤„ç†å•ä¸ªæ ·æœ¬çš„æƒ…å†µ
                    if batch_size == 1:
                        if model_features.ndim == 1:
                            sample_feature = model_features
                        else:
                            sample_feature = model_features.flatten()
                    else:
                        if model_features.ndim == 1:
                            sample_feature = model_features
                        else:
                            sample_feature = model_features[j].flatten()
                    
                    sample_features.append(sample_feature)
                
                # æ‹¼æ¥å½“å‰æ ·æœ¬çš„æ‰€æœ‰ç‰¹å¾
                concatenated_features = np.concatenate(sample_features)
                all_vectors[filename] = concatenated_features
                processed_count += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            avg_time_per_image = batch_time/len(valid_paths) if len(valid_paths) > 0 else 0
            print(f"ğŸ“Š æ‰¹æ¬¡ {i//batch_size + 1}: {len(valid_paths)} å¼ å›¾åƒï¼Œ"
                  f"è€—æ—¶ {batch_time:.3f}ç§’ "
                  f"(å¹³å‡ {avg_time_per_image:.3f}ç§’/å¼ )")

        return {
            "vectors": all_vectors,
            "total_time": total_time,
            "processed_count": processed_count,
            "failed_count": failed_count,
            "avg_time_per_image": total_time / processed_count if processed_count > 0 else 0,
            "device": str(self.device),
            "models": self.model_names,
            "feature_dim": self.total_feature_dim,
            "num_models": len(self.model_names)
        }

    def get_device_info(self) -> Dict[str, Union[str, float]]:
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        info = {
            "device_type": self.device.type,
            "device_name": str(self.device)
        }
        
        if self.device.type == 'cuda':
            info.update({
                "gpu_name": torch.cuda.get_device_name(0),
                "gpu_memory_total": torch.cuda.get_device_properties(0).total_memory / 1024**3,
                "gpu_memory_allocated": torch.cuda.memory_allocated(0) / 1024**3,
                "gpu_memory_reserved": torch.cuda.memory_reserved(0) / 1024**3,
            })
        
        return info

def create_test_images(output_dir: str, count: int = 10):
    """
    åˆ›å»ºæµ‹è¯•å›¾åƒæ–‡ä»¶
    
    Args:
        output_dir (str): è¾“å‡ºç›®å½•
        count (int): åˆ›å»ºå›¾åƒæ•°é‡
    """
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"æ­£åœ¨åˆ›å»º {count} å¼ æµ‹è¯•å›¾åƒåˆ° {output_dir}")
    
    for i in range(count):
        # åˆ›å»ºéšæœºå½©è‰²å›¾åƒ
        img_array = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
        img = Image.fromarray(img_array)
        
        # æ·»åŠ ä¸€äº›ç®€å•çš„å‡ ä½•å½¢çŠ¶
        from PIL import ImageDraw
        draw = ImageDraw.Draw(img)
        
        # éšæœºç»˜åˆ¶çŸ©å½¢å’Œåœ†å½¢
        for _ in range(3):
            x1, y1 = np.random.randint(0, 150, 2)
            x2, y2 = x1 + np.random.randint(20, 74), y1 + np.random.randint(20, 74)
            color = tuple(np.random.randint(0, 255, 3))
            
            if np.random.random() > 0.5:
                draw.rectangle([x1, y1, x2, y2], fill=color)
            else:
                draw.ellipse([x1, y1, x2, y2], fill=color)
        
        # ä¿å­˜å›¾åƒ
        img.save(os.path.join(output_dir, f"test_image_{i+1:03d}.jpg"))
    
    print(f"âœ“ å·²åˆ›å»º {count} å¼ æµ‹è¯•å›¾åƒ")

def main():
    parser = argparse.ArgumentParser(
        description="å›¾åƒå‘é‡åŒ–å·¥å…· - æ”¯æŒCPU/GPUæ€§èƒ½å¯¹æ¯”",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¤„ç†å•å¼ å›¾åƒ (CPU, ä½¿ç”¨æ‰€æœ‰æ¨¡å‹)
  python examples/image_vectorizer.py --image_path image.jpg --device cpu
  
  # æ‰¹é‡å¤„ç†ç›®å½• (GPU, ä½¿ç”¨æ‰€æœ‰æ¨¡å‹)
  python examples/image_vectorizer.py --image_path ./images/ --device gpu --batch_size 16
  
  # ä½¿ç”¨ç‰¹å®šæ¨¡å‹ç»„åˆ (ç»å…¸æ¨¡å‹)
  python examples/image_vectorizer.py --image_path image.jpg --models resnet152,vit_l_16
  
  # ä½¿ç”¨OpenAI CLIPæ¨¡å‹
  python examples/image_vectorizer.py --image_path image.jpg --models openai_clip_vit_b32
  
  # ä½¿ç”¨OpenCLIPå¢å¼ºæ¨¡å‹
  python examples/image_vectorizer.py --image_path image.jpg --models open_clip_vit_h14_laion2b
  
  # æ··åˆæ¨¡å‹é›†æˆ (CLIP + ç»å…¸æ¨¡å‹)
  python examples/image_vectorizer.py --image_path image.jpg --models openai_clip_vit_l14,resnet152
  
  # ä½¿ç”¨å•ä¸ªæ¨¡å‹ï¼ˆå‘åå…¼å®¹ï¼‰
  python examples/image_vectorizer.py --image_path image.jpg --model resnet152
  
  # åˆ›å»ºæµ‹è¯•å›¾åƒ
  python examples/image_vectorizer.py --create_test_images ./test_images --count 20
  
  # æ€§èƒ½å¯¹æ¯”æµ‹è¯• (å¤šæ¨¡å‹é›†æˆ vs CPU/GPU)
  python examples/image_vectorizer.py --image_path ./test_images/ --device cpu --batch_size 4
  python examples/image_vectorizer.py --image_path ./test_images/ --device gpu --batch_size 4
        """
    )
    
    parser.add_argument("--image_path", type=str, 
                       help="å•å¼ å›¾åƒè·¯å¾„æˆ–å›¾åƒç›®å½•è·¯å¾„")
    parser.add_argument("--device", type=str, default="cpu", 
                       help="è®¡ç®—è®¾å¤‡ (ä¾‹å¦‚: cpu, cuda, mps, gpu) (é»˜è®¤: cpu)")
    parser.add_argument("--batch_size", type=int, default=8, 
                       help="æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 8)")
    parser.add_argument("--models", type=str, default="resnet152",
                       help="è¦ä½¿ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”\n"
                            "ç»å…¸æ¨¡å‹: resnet152,efficientnet_b7,vit_l_16\n"
                            "OpenAI CLIP: openai_clip_vit_b32,openai_clip_vit_l14\n"
                            "OpenCLIP: open_clip_vit_b32_laion400m,open_clip_vit_h14_laion2b\n"
                            "(é»˜è®¤: resnet152)")
    parser.add_argument("--model", type=str, default=None,
                       help="å•ä¸ªæ¨¡å‹åç§°ï¼ˆå‘åå…¼å®¹ï¼Œå»ºè®®ä½¿ç”¨--modelsï¼‰")
    parser.add_argument("--create_test_images", type=str,
                       help="åˆ›å»ºæµ‹è¯•å›¾åƒåˆ°æŒ‡å®šç›®å½•")
    parser.add_argument("--count", type=int, default=10,
                       help="åˆ›å»ºæµ‹è¯•å›¾åƒçš„æ•°é‡ (é»˜è®¤: 10)")
    
    args = parser.parse_args()

    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    if args.create_test_images:
        create_test_images(args.create_test_images, args.count)
        return

    # æ£€æŸ¥å‚æ•°
    if not args.image_path:
        parser.print_help()
        return

    try:
        print("ğŸš€ å›¾åƒå‘é‡åŒ–å·¥å…·å¯åŠ¨")
        print("=" * 50)
        
        # å¤„ç†æ¨¡å‹å‚æ•°
        if args.model:
            # å‘åå…¼å®¹ï¼šä½¿ç”¨å•ä¸ªæ¨¡å‹
            model_names = [args.model]
        else:
            # ä½¿ç”¨å¤šæ¨¡å‹åˆ—è¡¨
            model_names = [m.strip() for m in args.models.split(',') if m.strip()]
        
        print(f"é€‰æ‹©çš„æ¨¡å‹: {model_names}")
        
        # åˆå§‹åŒ–å‘é‡åŒ–å™¨
        vectorizer = ImageVectorizer(device=args.device, model_names=model_names)
        
        # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
        device_info = vectorizer.get_device_info()
        print("\nğŸ“± è®¾å¤‡ä¿¡æ¯:")
        if device_info["device_type"] == "cuda":
            print(f"  GPU: {device_info['gpu_name']}")
            print(f"  æ€»å†…å­˜: {device_info['gpu_memory_total']:.1f} GB")
        else:
            print(f"  è®¾å¤‡: CPU")

        if os.path.isdir(args.image_path):
            # å¤„ç†ç›®å½•ä¸­çš„å›¾åƒ
            print(f"\nğŸ“‚ æ‰¹é‡å¤„ç†æ¨¡å¼")
            print(f"ç›®å½•è·¯å¾„: {args.image_path}")
            
            results = vectorizer.vectorize_directory(args.image_path, args.batch_size)
            
            if results:
                print("\n" + "=" * 50)
                print("ğŸ‰ å‘é‡åŒ–å®Œæˆ!")
                print("-" * 30)
                print(f"è®¾å¤‡: {results['device']}")
                print(f"æ¨¡å‹: {results['models']} ({results['num_models']}ä¸ªæ¨¡å‹)")
                print(f"ç‰¹å¾ç»´åº¦: {results['feature_dim']}")
                print(f"æˆåŠŸå¤„ç†: {results['processed_count']} å¼ å›¾åƒ")
                if results['failed_count'] > 0:
                    print(f"å¤±è´¥: {results['failed_count']} å¼ å›¾åƒ")
                print(f"æ€»è€—æ—¶: {results['total_time']:.3f} ç§’")
                print(f"å¹³å‡æ¯å¼ : {results['avg_time_per_image']:.3f} ç§’")
                if results['total_time'] > 0:
                    print(f"å¤„ç†é€Ÿåº¦: {results['processed_count']/results['total_time']:.2f} å¼ /ç§’")
                else:
                    print(f"å¤„ç†é€Ÿåº¦: N/A (æ— æœ‰æ•ˆå¤„ç†)")
                
                # æ˜¾ç¤ºç¬¬ä¸€ä¸ªå‘é‡ç¤ºä¾‹
                if results['vectors']:
                    first_image = list(results['vectors'].keys())[0]
                    first_vector = results['vectors'][first_image]
                    print(f"\nğŸ“Š ç¤ºä¾‹ç‰¹å¾å‘é‡ ('{first_image}'):")
                    print(f"  å½¢çŠ¶: {first_vector.shape}")
                    print(f"  å‰10ä¸ªå…ƒç´ : {first_vector[:10]}")
                    print(f"  ç»Ÿè®¡: å‡å€¼={first_vector.mean():.4f}, "
                          f"æ ‡å‡†å·®={first_vector.std():.4f}")

        elif os.path.isfile(args.image_path):
            # å¤„ç†å•å¼ å›¾åƒ
            print(f"\nğŸ–¼ï¸ å•å›¾åƒå¤„ç†æ¨¡å¼")
            print(f"å›¾åƒè·¯å¾„: {args.image_path}")
            
            start_time = time.time()
            vector = vectorizer.vectorize_image(args.image_path)
            end_time = time.time()
            
            processing_time = end_time - start_time
            
            print("\n" + "=" * 50)
            print("ğŸ‰ å‘é‡åŒ–å®Œæˆ!")
            print("-" * 30)
            print(f"è®¾å¤‡: {vectorizer.device}")
            print(f"æ¨¡å‹: {vectorizer.model_names} ({len(vectorizer.model_names)}ä¸ªæ¨¡å‹)")
            print(f"å¤„ç†æ—¶é—´: {processing_time:.4f} ç§’")
            print(f"ç‰¹å¾å‘é‡å½¢çŠ¶: {vector.shape}")
            print(f"å‰10ä¸ªå…ƒç´ : {vector[:10]}")
            print(f"ç»Ÿè®¡ä¿¡æ¯: å‡å€¼={vector.mean():.4f}, æ ‡å‡†å·®={vector.std():.4f}")

        else:
            print(f"âŒ é”™è¯¯: è·¯å¾„ '{args.image_path}' ä¸æ˜¯æœ‰æ•ˆçš„æ–‡ä»¶æˆ–ç›®å½•")

    except KeyboardInterrupt:
        print("\nâš¡ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()