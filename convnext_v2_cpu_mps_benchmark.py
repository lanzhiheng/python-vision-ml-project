#!/usr/bin/env python3
"""
å¤šæ¨¡å‹è§†è§‰ AI æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·

è¿™ä¸ªè„šæœ¬å¯ä»¥æµ‹è¯•å¤šç§è§†è§‰æ¨¡å‹åœ¨ä¸åŒè®¡ç®—è®¾å¤‡ä¸Šçš„æ€§èƒ½å·®å¼‚ï¼š
- CPU: åˆ©ç”¨æ‰€æœ‰CPUæ ¸å¿ƒè¿›è¡Œå¤šçº¿ç¨‹æ¨ç†
- MPS (Metal Performance Shaders): åˆ©ç”¨ Apple Silicon çš„ GPU åŠ é€Ÿ
- CUDA: åˆ©ç”¨ NVIDIA GPU åŠ é€Ÿ

æ”¯æŒçš„æ¨¡å‹åŒ…æ‹¬ï¼š
- ConvNeXt V2-Large: ç°ä»£å·ç§¯ç½‘ç»œ
- ResNet-50: ç»å…¸æ®‹å·®ç½‘ç»œ  
- OpenCLIP ViT-G14: å¤§å‹è§†è§‰Transformer
- OpenAI CLIP ViT-L/14@336px: OpenAI CLIPæ¨¡å‹

æµ‹è¯•æŒ‡æ ‡åŒ…æ‹¬ï¼š
- å•å¼ å›¾åƒå¤„ç†æ—¶é—´
- æ‰¹å¤„ç†ååé‡
- å†…å­˜ä½¿ç”¨æƒ…å†µ
- æ¨¡å‹åŠ è½½æ—¶é—´
- ç‰¹å¾æå–è´¨é‡å¯¹æ¯”

æ”¯æŒåŠŸèƒ½ï¼š
- è‡ªåŠ¨è®¾å¤‡æ£€æµ‹å’Œå…¼å®¹æ€§æ£€æŸ¥
- åŠ¨æ€æ‰¹å¤„ç†å¤§å°ä¼˜åŒ–
- æ··åˆç²¾åº¦æ¨ç†ï¼ˆMPS/CUDAï¼‰
- è¯¦ç»†çš„æ€§èƒ½åˆ†ææŠ¥å‘Š
- å¯è§†åŒ–æ€§èƒ½å¯¹æ¯”å›¾è¡¨
- å¤šæ¨¡å‹å¯¹æ¯”åˆ†æ
"""

import os
import sys
import time
import argparse
import json
import statistics
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
import numpy as np
from PIL import Image

import torch
import torch.nn.functional as F
import psutil
import warnings

# ç¦ç”¨ä¸€äº›è­¦å‘Šä»¥ä¿æŒè¾“å‡ºæ•´æ´
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

class VisionModelBenchmark:
    """å¤šæ¨¡å‹è§†è§‰ AI æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self, test_image_dir: str = "benchmark_test_images", num_test_images: int = 50):
        """
        åˆå§‹åŒ–åŸºå‡†æµ‹è¯•
        
        Args:
            test_image_dir: æµ‹è¯•å›¾åƒç›®å½•
            num_test_images: æµ‹è¯•å›¾åƒæ•°é‡
        """
        self.test_image_dir = test_image_dir
        self.num_test_images = num_test_images
        
        # æ”¯æŒçš„æ¨¡å‹é…ç½®
        self.model_configs = {
            'convnext_v2_large': {
                'name': 'ConvNeXt V2-Large',
                'hf_model_name': 'facebook/convnextv2-large-1k-224',
                'input_size': 224,
                'expected_feature_dim': 768,
                'model_type': 'transformers',
                'description': 'ConvNeXt V2-Large ç°ä»£å·ç§¯ç½‘ç»œ'
            },
            'resnet50': {
                'name': 'ResNet-50',
                'hf_model_name': 'microsoft/resnet-50',
                'input_size': 224,
                'expected_feature_dim': 2048,
                'model_type': 'transformers',
                'description': 'ResNet-50 ç»å…¸æ®‹å·®ç½‘ç»œ'
            },
            # å¯èƒ½åç»­ä¼šä½¿ç”¨çš„æ¨¡å‹
            'open_clip_vit_g14': {
                'name': 'OpenCLIP ViT-G/14',
                'model_name': 'ViT-g-14',
                'pretrained': 'laion2b_s34b_b88k',
                'input_size': 224,
                'expected_feature_dim': 1024,
                'model_type': 'open_clip',
                'description': 'OpenCLIP ViT-G/14 å¤§å‹è§†è§‰Transformer'
            },
            # ç›®å‰æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹
            'openai_clip_vit_l14_336': {
                'name': 'OpenAI CLIP ViT-L/14@336px',
                'model_name': 'ViT-L/14@336px',
                'input_size': 336,
                'expected_feature_dim': 768,
                'model_type': 'clip',
                'description': 'OpenAI CLIP ViT-L/14@336px'
            }
        }
        
        # æ€§èƒ½ç»Ÿè®¡
        self.results = {}
        self.device_info = {}
        
        # æ£€æŸ¥ä¾èµ–
        self._check_dependencies()
        
        # æ£€æµ‹å¯ç”¨è®¾å¤‡
        self._detect_devices()
        
        # å‡†å¤‡æµ‹è¯•å›¾åƒ
        self._prepare_test_images()
    
    def _check_dependencies(self):
        """æ£€æŸ¥å¿…è¦çš„ä¾èµ–åº“"""
        print("ğŸ” æ£€æŸ¥ä¾èµ–åº“...")
        
        missing_deps = []
        optional_missing = []
        
        try:
            import torch
            print(f"  âœ… PyTorch: {torch.__version__}")
        except ImportError:
            missing_deps.append("torch")
        
        try:
            import transformers
            print(f"  âœ… Transformers: {transformers.__version__}")
        except ImportError:
            missing_deps.append("transformers")
        
        try:
            from PIL import Image
            print(f"  âœ… Pillow: Available")
        except ImportError:
            missing_deps.append("Pillow")
        
        try:
            import psutil
            print(f"  âœ… PSUtil: {psutil.__version__}")
        except ImportError:
            missing_deps.append("psutil")
        
        # æ£€æŸ¥å¯é€‰ä¾èµ–
        try:
            import open_clip
            print(f"  âœ… OpenCLIP: {open_clip.__version__}")
        except ImportError:
            optional_missing.append("open_clip")
            print(f"  âš ï¸ OpenCLIP: æœªå®‰è£… (å½±å“ OpenCLIP æ¨¡å‹)")
        
        try:
            import clip
            print(f"  âœ… OpenAI CLIP: Available")
        except ImportError:
            optional_missing.append("clip")
            print(f"  âš ï¸ OpenAI CLIP: æœªå®‰è£… (å½±å“ OpenAI CLIP æ¨¡å‹)")
        
        if missing_deps:
            print(f"âŒ ç¼ºå°‘å¿…è¦ä¾èµ–åº“: {', '.join(missing_deps)}")
            print("è¯·è¿è¡Œ: pip install torch transformers Pillow psutil")
            sys.exit(1)
        
        if optional_missing:
            print(f"ğŸ’¡ å¯é€‰ä¾èµ–åº“æœªå®‰è£…: {', '.join(optional_missing)}")
            print("å®Œæ•´åŠŸèƒ½è¯·è¿è¡Œ: pip install open_clip_torch git+https://github.com/openai/CLIP.git")
        
        print("âœ… å¿…è¦ä¾èµ–åº“å·²å®‰è£…")
    
    def _detect_devices(self):
        """æ£€æµ‹å¯ç”¨çš„è®¡ç®—è®¾å¤‡"""
        print("\nğŸ–¥ï¸ æ£€æµ‹è®¡ç®—è®¾å¤‡...")
        
        devices = []
        
        # CPU å§‹ç»ˆå¯ç”¨
        cpu_info = {
            'device': 'cpu',
            'name': f'CPU ({psutil.cpu_count()} cores)',
            'memory': f'{psutil.virtual_memory().total / (1024**3):.1f} GB',
            'available': True
        }
        devices.append('cpu')
        self.device_info['cpu'] = cpu_info
        print(f"  âœ… CPU: {psutil.cpu_count()} æ ¸å¿ƒ, {psutil.virtual_memory().total / (1024**3):.1f} GB RAM")
        
        # æ£€æŸ¥ MPS (Apple Silicon)
        if torch.backends.mps.is_available():
            mps_info = {
                'device': 'mps',
                'name': 'Apple Silicon GPU (MPS)',
                'memory': 'Unified Memory',
                'available': True
            }
            devices.append('mps')
            self.device_info['mps'] = mps_info
            print(f"  âœ… MPS: Apple Silicon GPU å¯ç”¨")
        else:
            print(f"  âŒ MPS: ä¸å¯ç”¨ï¼ˆé Apple Silicon æˆ–ç‰ˆæœ¬ä¸æ”¯æŒï¼‰")
        
        # æ£€æŸ¥ CUDA
        if torch.cuda.is_available():
            cuda_info = {
                'device': 'cuda',
                'name': f'CUDA GPU ({torch.cuda.get_device_name()})',
                'memory': f'{torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB',
                'available': True
            }
            devices.append('cuda')
            self.device_info['cuda'] = cuda_info
            print(f"  âœ… CUDA: {torch.cuda.get_device_name()}")
        else:
            print(f"  âŒ CUDA: ä¸å¯ç”¨")
        
        self.available_devices = devices
        print(f"\nğŸ“Š æ€»å…±æ£€æµ‹åˆ° {len(devices)} ä¸ªå¯ç”¨è®¾å¤‡: {', '.join(devices)}")
    
    def _prepare_test_images(self):
        """å‡†å¤‡æµ‹è¯•å›¾åƒ"""
        print(f"\nğŸ“ å‡†å¤‡æµ‹è¯•å›¾åƒ...")
        
        if not os.path.exists(self.test_image_dir):
            print(f"åˆ›å»ºæµ‹è¯•å›¾åƒç›®å½•: {self.test_image_dir}")
            os.makedirs(self.test_image_dir)
            self._create_synthetic_images()
        else:
            # æ£€æŸ¥ç°æœ‰å›¾åƒ
            image_files = list(Path(self.test_image_dir).glob("*.jpg"))
            if len(image_files) < self.num_test_images:
                print(f"å›¾åƒæ•°é‡ä¸è¶³ ({len(image_files)}/{self.num_test_images})ï¼Œåˆ›å»ºæ›´å¤šæµ‹è¯•å›¾åƒ...")
                self._create_synthetic_images()
            else:
                print(f"âœ… æµ‹è¯•å›¾åƒå·²å‡†å¤‡: {len(image_files)} å¼ ")
        
        # è·å–å›¾åƒè·¯å¾„åˆ—è¡¨
        self.image_paths = list(Path(self.test_image_dir).glob("*.jpg"))[:self.num_test_images]
        print(f"âœ… ä½¿ç”¨ {len(self.image_paths)} å¼ æµ‹è¯•å›¾åƒ")
    
    def _create_synthetic_images(self):
        """åˆ›å»ºåˆæˆæµ‹è¯•å›¾åƒ"""
        print(f"ğŸ¨ åˆ›å»º {self.num_test_images} å¼ åˆæˆæµ‹è¯•å›¾åƒ...")
        
        for i in range(self.num_test_images):
            # åˆ›å»ºéšæœºå½©è‰²å›¾åƒ
            img = Image.new('RGB', (512, 512))
            pixels = []
            
            for y in range(512):
                for x in range(512):
                    # åˆ›å»ºæ¸å˜å’Œå™ªå£°æ•ˆæœ
                    r = int((x / 512) * 255) ^ (i * 7)
                    g = int((y / 512) * 255) ^ (i * 11)
                    b = int(((x + y) / 1024) * 255) ^ (i * 13)
                    pixels.append((r % 256, g % 256, b % 256))
            
            img.putdata(pixels)
            img_path = os.path.join(self.test_image_dir, f"test_{i+1:04d}.jpg")
            img.save(img_path, quality=95)
        
        print(f"âœ… åˆ›å»ºå®Œæˆ: {self.num_test_images} å¼ æµ‹è¯•å›¾åƒ")
    
    def _load_model(self, model_key: str, device: str) -> Tuple[torch.nn.Module, callable]:
        """
        åŠ è½½æŒ‡å®šæ¨¡å‹
        
        Args:
            model_key: æ¨¡å‹é…ç½®é”®å
            device: ç›®æ ‡è®¾å¤‡ ('cpu', 'mps', 'cuda')
            
        Returns:
            (model, preprocessor): æ¨¡å‹å’Œé¢„å¤„ç†å™¨
        """
        if model_key not in self.model_configs:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_key}")
        
        config = self.model_configs[model_key]
        print(f"ğŸ“¦ åŠ è½½ {config['name']} æ¨¡å‹åˆ° {device.upper()}...")
        
        start_time = time.time()
        
        try:
            if config['model_type'] == 'transformers':
                model, preprocessor = self._load_transformers_model(config, device)
            elif config['model_type'] == 'open_clip':
                model, preprocessor = self._load_open_clip_model(config, device)
            elif config['model_type'] == 'clip':
                model, preprocessor = self._load_openai_clip_model(config, device)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {config['model_type']}")
            
            # æµ‹è¯•æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
            print(f"  æ­£åœ¨éªŒè¯æ¨¡å‹åŠŸèƒ½...")
            input_size = config['input_size']
            test_input = torch.randn(1, 3, input_size, input_size).to(device)
            
            with torch.no_grad():
                if config['model_type'] == 'transformers':
                    test_output = model(test_input)
                    if hasattr(test_output, 'last_hidden_state'):
                        feature_dim = test_output.last_hidden_state.shape[-1]
                    elif hasattr(test_output, 'pooler_output'):
                        feature_dim = test_output.pooler_output.shape[-1]
                    else:
                        feature_dim = test_output.shape[-1]
                elif config['model_type'] in ['open_clip', 'clip']:
                    test_output = model.encode_image(test_input)
                    feature_dim = test_output.shape[-1]
                
                # åŒæ­¥è®¾å¤‡
                if device == 'mps' and hasattr(torch.mps, 'synchronize'):
                    torch.mps.synchronize()
                elif device == 'cuda':
                    torch.cuda.synchronize()
            
            # éªŒè¯è¾“å‡ºç»´åº¦
            expected_dim = config['expected_feature_dim']
            if feature_dim != expected_dim:
                print(f"  âš ï¸ ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: {feature_dim} != {expected_dim}")
            else:
                print(f"  âœ… ç‰¹å¾ç»´åº¦éªŒè¯é€šè¿‡: {feature_dim}")
            
            load_time = time.time() - start_time
            print(f"  âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f} ç§’")
            
            return model, preprocessor
            
        except Exception as e:
            print(f"  âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _load_transformers_model(self, config: Dict, device: str) -> Tuple[torch.nn.Module, callable]:
        """åŠ è½½ Transformers æ¨¡å‹"""
        from transformers import AutoModel, AutoImageProcessor
        
        print(f"  æ­£åœ¨ä¸‹è½½æ¨¡å‹æƒé‡: {config['hf_model_name']}")
        model = AutoModel.from_pretrained(config['hf_model_name'])
        preprocessor = AutoImageProcessor.from_pretrained(config['hf_model_name'])
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
        model.eval()
        model = model.to(device)
        
        return model, preprocessor
    
    def _load_open_clip_model(self, config: Dict, device: str) -> Tuple[torch.nn.Module, callable]:
        """åŠ è½½ OpenCLIP æ¨¡å‹"""
        try:
            import open_clip
        except ImportError:
            raise ImportError("OpenCLIP æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install open_clip_torch")
        
        print(f"  æ­£åœ¨ä¸‹è½½æ¨¡å‹: {config['model_name']} with {config['pretrained']}")
        model, _, preprocessor = open_clip.create_model_and_transforms(
            config['model_name'], 
            pretrained=config['pretrained'],
            device=device
        )
        
        model.eval()
        return model, preprocessor
    
    def _load_openai_clip_model(self, config: Dict, device: str) -> Tuple[torch.nn.Module, callable]:
        """åŠ è½½ OpenAI CLIP æ¨¡å‹"""
        try:
            import clip
        except ImportError:
            raise ImportError("OpenAI CLIP æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install git+https://github.com/openai/CLIP.git")
        
        print(f"  æ­£åœ¨ä¸‹è½½æ¨¡å‹: {config['model_name']}")
        model, preprocessor = clip.load(config['model_name'], device=device)
        
        model.eval()
        return model, preprocessor
    
    def _preprocess_images(self, image_paths: List[str], preprocessor, model_type: str, input_size: int) -> torch.Tensor:
        """
        é¢„å¤„ç†å›¾åƒæ‰¹æ¬¡
        
        Args:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            preprocessor: é¢„å¤„ç†å™¨
            model_type: æ¨¡å‹ç±»å‹
            input_size: è¾“å…¥å°ºå¯¸
            
        Returns:
            é¢„å¤„ç†åçš„å¼ é‡
        """
        images = []
        
        for path in image_paths:
            try:
                img = Image.open(path).convert('RGB')
                images.append(img)
            except Exception as e:
                print(f"  âš ï¸ åŠ è½½å›¾åƒå¤±è´¥ {path}: {e}")
                # åˆ›å»ºé»˜è®¤å›¾åƒ
                images.append(Image.new('RGB', (input_size, input_size), color='red'))
        
        if model_type == 'transformers':
            # ä½¿ç”¨ transformers é¢„å¤„ç†å™¨
            inputs = preprocessor(images, return_tensors="pt")
            return inputs['pixel_values']
        elif model_type in ['open_clip', 'clip']:
            # ä½¿ç”¨ CLIP é¢„å¤„ç†å™¨
            if model_type == 'open_clip':
                # OpenCLIP é¢„å¤„ç†å™¨å¯ä»¥ç›´æ¥å¤„ç†å›¾åƒåˆ—è¡¨
                return torch.stack([preprocessor(img) for img in images])
            else:
                # OpenAI CLIP é¢„å¤„ç†å™¨
                return torch.stack([preprocessor(img) for img in images])
    
    def _extract_features(self, model: torch.nn.Module, input_tensor: torch.Tensor, 
                         device: str, model_type: str, use_mixed_precision: bool = False) -> torch.Tensor:
        """
        æå–ç‰¹å¾
        
        Args:
            model: æ¨¡å‹
            input_tensor: è¾“å…¥å¼ é‡
            device: è®¾å¤‡
            model_type: æ¨¡å‹ç±»å‹
            use_mixed_precision: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦
            
        Returns:
            ç‰¹å¾å‘é‡
        """
        model.eval()
        
        with torch.no_grad():
            if use_mixed_precision and device in ['mps', 'cuda']:
                # ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç†
                if device == 'mps':
                    # MPS æ”¯æŒè‡ªåŠ¨æ··åˆç²¾åº¦
                    with torch.autocast(device_type='mps', dtype=torch.float16):
                        features = self._forward_model(model, input_tensor, model_type)
                else:
                    # CUDA æ··åˆç²¾åº¦
                    with torch.autocast(device_type='cuda', dtype=torch.float16):
                        features = self._forward_model(model, input_tensor, model_type)
            else:
                # æ ‡å‡†ç²¾åº¦æ¨ç†
                features = self._forward_model(model, input_tensor, model_type)
            
        return features
    
    def _forward_model(self, model: torch.nn.Module, input_tensor: torch.Tensor, model_type: str) -> torch.Tensor:
        """æ ¹æ®æ¨¡å‹ç±»å‹æ‰§è¡Œå‰å‘ä¼ æ’­"""
        if model_type == 'transformers':
            outputs = model(input_tensor)
            
            if hasattr(outputs, 'last_hidden_state'):
                # ConvNeXt V2 ç±»å‹è¾“å‡º
                last_hidden_state = outputs.last_hidden_state
                # å…¨å±€å¹³å‡æ± åŒ–: å¯¹åºåˆ—ç»´åº¦æ±‚å¹³å‡
                features = torch.mean(last_hidden_state, dim=1)
            elif hasattr(outputs, 'pooler_output'):
                # ResNet ç±»å‹è¾“å‡º
                features = outputs.pooler_output
            else:
                # ç›´æ¥ç‰¹å¾è¾“å‡º
                features = outputs
        
        elif model_type in ['open_clip', 'clip']:
            # CLIP æ¨¡å‹ä½¿ç”¨ encode_image
            features = model.encode_image(input_tensor)
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        return features

    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        available = []
        
        for model_key, config in self.model_configs.items():
            if config['model_type'] == 'transformers':
                available.append(model_key)
            elif config['model_type'] == 'open_clip':
                try:
                    import open_clip
                    available.append(model_key)
                except ImportError:
                    continue
            elif config['model_type'] == 'clip':
                try:
                    import clip
                    available.append(model_key)
                except ImportError:
                    continue
        
        return available

    def benchmark_single_image(self, model_key: str, device: str, num_runs: int = 10) -> Dict:
        """
        å•å¼ å›¾åƒå¤„ç†æ€§èƒ½æµ‹è¯•
        
        Args:
            model_key: æ¨¡å‹é…ç½®é”®å
            device: æµ‹è¯•è®¾å¤‡
            num_runs: è¿è¡Œæ¬¡æ•°
            
        Returns:
            æ€§èƒ½ç»Ÿè®¡ç»“æœ
        """
        config = self.model_configs[model_key]
        print(f"\nğŸ” å•å¼ å›¾åƒå¤„ç†æ€§èƒ½æµ‹è¯• - {config['name']} on {device.upper()}")
        print("-" * 50)
        
        # åŠ è½½æ¨¡å‹
        model, preprocessor = self._load_model(model_key, device)
        
        # å‡†å¤‡æµ‹è¯•å›¾åƒ
        test_image_path = self.image_paths[0]
        
        # é¢„çƒ­
        print(f"  ğŸ”¥ æ¨¡å‹é¢„çƒ­...")
        input_tensor = self._preprocess_images([test_image_path], preprocessor, 
                                             config['model_type'], config['input_size']).to(device)
        
        for _ in range(3):
            features = self._extract_features(model, input_tensor, device, config['model_type'])
            if device == 'mps' and hasattr(torch.mps, 'synchronize'):
                torch.mps.synchronize()
        
        # æ­£å¼æµ‹è¯•
        print(f"  â±ï¸ å¼€å§‹ {num_runs} æ¬¡æµ‹è¯•...")
        processing_times = []
        memory_usage = []
        
        for i in range(num_runs):
            # è·å–è¿›ç¨‹å¯¹è±¡ï¼ˆä»…CPUéœ€è¦ï¼‰
            if device == 'cpu':
                process = psutil.Process()
            
            # è®¡æ—¶å¼€å§‹
            start_time = time.time()
            
            # ç‰¹å¾æå–
            features = self._extract_features(model, input_tensor, device, config['model_type'])
            
            # åŒæ­¥ï¼ˆç¡®ä¿è®¡ç®—å®Œæˆï¼‰
            if device == 'mps' and hasattr(torch.mps, 'synchronize'):
                torch.mps.synchronize()
            elif device == 'cuda':
                torch.cuda.synchronize()
            
            # è®¡æ—¶ç»“æŸ
            end_time = time.time()
            
            processing_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            processing_times.append(processing_time)
            
            # å†…å­˜ä½¿ç”¨ç›‘æ§ï¼ˆç›‘æµ‹æ€»å†…å­˜ä½¿ç”¨é‡ï¼‰
            if device == 'cpu':
                # CPU å†…å­˜ç›‘æ§ï¼šå½“å‰è¿›ç¨‹æ€»å†…å­˜ä½¿ç”¨
                current_memory = process.memory_info().rss / (1024**2)  # MB
                memory_usage.append(current_memory)
            elif device == 'mps':
                current_memory = torch.mps.current_allocated_memory() / (1024**2)  # MB
                memory_usage.append(current_memory)
            elif device == 'cuda':
                current_memory = torch.cuda.memory_allocated() / (1024**2)  # MB
                memory_usage.append(current_memory)
            
            if (i + 1) % 5 == 0:
                print(f"    è¿›åº¦: {i + 1}/{num_runs}")
        
        # ç»Ÿè®¡ç»“æœ
        result = {
            'model_key': model_key,
            'device': device,
            'model_name': config['name'],
            'test_type': 'single_image',
            'num_runs': num_runs,
            'processing_times_ms': processing_times,
            'avg_time_ms': statistics.mean(processing_times),
            'median_time_ms': statistics.median(processing_times),
            'std_time_ms': statistics.stdev(processing_times) if len(processing_times) > 1 else 0,
            'min_time_ms': min(processing_times),
            'max_time_ms': max(processing_times),
            'memory_usage_mb': memory_usage,
            'avg_memory_mb': statistics.mean(memory_usage) if memory_usage else 0,
            'feature_shape': features.shape,
            'feature_sample': features[0][:5].cpu().tolist() if features.numel() > 0 else []
        }
        
        # æ‰“å°ç»“æœ
        print(f"  ğŸ“Š ç»“æœç»Ÿè®¡:")
        print(f"    å¹³å‡å¤„ç†æ—¶é—´: {result['avg_time_ms']:.2f} Â± {result['std_time_ms']:.2f} ms")
        print(f"    ä¸­ä½æ•°æ—¶é—´: {result['median_time_ms']:.2f} ms")
        print(f"    æ—¶é—´èŒƒå›´: {result['min_time_ms']:.2f} - {result['max_time_ms']:.2f} ms")
        print(f"    å¹³å‡å†…å­˜ä½¿ç”¨: {result['avg_memory_mb']:.2f} MB")
        print(f"    ç‰¹å¾å½¢çŠ¶: {result['feature_shape']}")
        
        return result
    
    def benchmark_batch_processing(self, model_key: str, device: str, batch_sizes: List[int] = None) -> Dict:
        """
        æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•
        
        Args:
            model_key: æ¨¡å‹é…ç½®é”®å
            device: æµ‹è¯•è®¾å¤‡
            batch_sizes: æµ‹è¯•çš„æ‰¹æ¬¡å¤§å°åˆ—è¡¨
            
        Returns:
            æ‰¹å¤„ç†æ€§èƒ½ç»“æœ
        """
        config = self.model_configs[model_key]
        print(f"\nğŸ“¦ æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯• - {config['name']} on {device.upper()}")
        print("-" * 50)
        
        if batch_sizes is None:
            # æ ¹æ®è®¾å¤‡ç±»å‹é€‰æ‹©åˆé€‚çš„æ‰¹æ¬¡å¤§å°
            if device == 'cpu':
                batch_sizes = [1, 2, 4, 8, 16]
            else:  # mps or cuda
                batch_sizes = [1, 4, 8, 16, 32]
        
        # åŠ è½½æ¨¡å‹
        model, preprocessor = self._load_model(model_key, device)
        
        batch_results = {}
        
        for batch_size in batch_sizes:
            print(f"\n  ğŸ§ª æµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size}")
            
            try:
                # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
                test_paths = self.image_paths[:batch_size]
                input_tensor = self._preprocess_images(test_paths, preprocessor, 
                                                     config['model_type'], config['input_size']).to(device)
                
                # é¢„çƒ­
                for _ in range(2):
                    features = self._extract_features(model, input_tensor, device, config['model_type'])
                    if device == 'mps' and hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                
                # æ€§èƒ½æµ‹è¯•
                num_runs = 5
                processing_times = []
                throughputs = []
                memory_usage = []
                
                for run in range(num_runs):
                    # è·å–è¿›ç¨‹å¯¹è±¡ï¼ˆä»…CPUéœ€è¦ï¼‰
                    if device == 'cpu':
                        process = psutil.Process()
                    
                    # è®¡æ—¶
                    start_time = time.time()
                    features = self._extract_features(model, input_tensor, device, config['model_type'])
                    
                    # åŒæ­¥
                    if device == 'mps' and hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                    elif device == 'cuda':
                        torch.cuda.synchronize()
                    
                    end_time = time.time()
                    
                    processing_time = end_time - start_time
                    throughput = batch_size / processing_time  # å›¾åƒ/ç§’
                    
                    processing_times.append(processing_time * 1000)  # ms
                    throughputs.append(throughput)
                    
                    # å†…å­˜ä½¿ç”¨ç›‘æ§ï¼ˆç›‘æµ‹æ€»å†…å­˜ä½¿ç”¨é‡ï¼‰
                    if device == 'cpu':
                        # CPU å†…å­˜ç›‘æ§ï¼šå½“å‰è¿›ç¨‹æ€»å†…å­˜ä½¿ç”¨
                        current_memory = process.memory_info().rss / (1024**2)  # MB
                        memory_usage.append(current_memory)
                    elif device == 'mps':
                        current_memory = torch.mps.current_allocated_memory() / (1024**2)  # MB
                        memory_usage.append(current_memory)
                    elif device == 'cuda':
                        current_memory = torch.cuda.memory_allocated() / (1024**2)  # MB
                        memory_usage.append(current_memory)
                
                # ç»Ÿè®¡ç»“æœ
                batch_result = {
                    'batch_size': batch_size,
                    'avg_time_ms': statistics.mean(processing_times),
                    'avg_throughput': statistics.mean(throughputs),
                    'std_throughput': statistics.stdev(throughputs) if len(throughputs) > 1 else 0,
                    'avg_memory_mb': statistics.mean(memory_usage) if memory_usage else 0,
                    'feature_shape': features.shape
                }
                
                batch_results[batch_size] = batch_result
                
                print(f"    å¹³å‡å¤„ç†æ—¶é—´: {batch_result['avg_time_ms']:.2f} ms")
                print(f"    å¹³å‡ååé‡: {batch_result['avg_throughput']:.2f} å›¾åƒ/ç§’")
                print(f"    å†…å­˜ä½¿ç”¨: {batch_result['avg_memory_mb']:.2f} MB")
                
            except Exception as e:
                print(f"    âŒ æ‰¹æ¬¡å¤§å° {batch_size} æµ‹è¯•å¤±è´¥: {e}")
                if "out of memory" in str(e).lower():
                    print(f"    å†…å­˜ä¸è¶³ï¼Œè·³è¿‡æ›´å¤§çš„æ‰¹æ¬¡å¤§å°")
                    break
        
        return {
            'model_key': model_key,
            'device': device,
            'model_name': config['name'],
            'test_type': 'batch_processing',
            'batch_results': batch_results
        }
    
    def run_comprehensive_benchmark(self, model_keys: List[str] = None, devices: List[str] = None) -> Dict:
        """
        è¿è¡Œç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            model_keys: è¦æµ‹è¯•çš„æ¨¡å‹é”®ååˆ—è¡¨ï¼ŒNone è¡¨ç¤ºæµ‹è¯•æ‰€æœ‰å¯ç”¨æ¨¡å‹
            devices: è¦æµ‹è¯•çš„è®¾å¤‡åˆ—è¡¨ï¼ŒNone è¡¨ç¤ºæµ‹è¯•æ‰€æœ‰å¯ç”¨è®¾å¤‡
            
        Returns:
            å®Œæ•´çš„åŸºå‡†æµ‹è¯•ç»“æœ
        """
        if model_keys is None:
            model_keys = self.get_available_models()
        
        if devices is None:
            devices = self.available_devices
        
        print(f"\nğŸš€ å¤šæ¨¡å‹ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 60)
        print(f"æµ‹è¯•è®¾å¤‡: {', '.join(d.upper() for d in devices)}")
        print(f"æµ‹è¯•æ¨¡å‹: {', '.join([self.model_configs[m]['name'] for m in model_keys])}")
        print(f"æµ‹è¯•å›¾åƒ: {len(self.image_paths)} å¼ ")
        
        results = {
            'model_configs': {k: self.model_configs[k] for k in model_keys},
            'device_info': self.device_info,
            'test_config': {
                'num_test_images': len(self.image_paths),
                'devices_tested': devices,
                'models_tested': model_keys
            },
            'single_image_results': {},
            'batch_processing_results': {},
            'comparison_analysis': {}
        }
        
        # å•å¼ å›¾åƒæµ‹è¯•
        print(f"\n" + "="*60)
        print(f"ç¬¬ä¸€é˜¶æ®µ: å•å¼ å›¾åƒå¤„ç†æ€§èƒ½æµ‹è¯•")
        print(f"="*60)
        
        for model_key in model_keys:
            results['single_image_results'][model_key] = {}
            for device in devices:
                try:
                    result = self.benchmark_single_image(model_key, device, num_runs=20)
                    results['single_image_results'][model_key][device] = result
                except Exception as e:
                    print(f"âŒ {model_key.upper()} on {device.upper()} å•å¼ å›¾åƒæµ‹è¯•å¤±è´¥: {e}")
                    results['single_image_results'][model_key][device] = {'error': str(e)}
        
        # æ‰¹å¤„ç†æµ‹è¯•
        print(f"\n" + "="*60)
        print(f"ç¬¬äºŒé˜¶æ®µ: æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•")
        print(f"="*60)
        
        for model_key in model_keys:
            results['batch_processing_results'][model_key] = {}
            for device in devices:
                try:
                    result = self.benchmark_batch_processing(model_key, device)
                    results['batch_processing_results'][model_key][device] = result
                except Exception as e:
                    print(f"âŒ {model_key.upper()} on {device.upper()} æ‰¹å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
                    results['batch_processing_results'][model_key][device] = {'error': str(e)}
        
        # å¯¹æ¯”åˆ†æ
        results['comparison_analysis'] = self._generate_comparison_analysis(results)
        
        return results
    
    def _generate_comparison_analysis(self, results: Dict) -> Dict:
        """ç”Ÿæˆå¯¹æ¯”åˆ†æ"""
        print(f"\n" + "="*60)
        print(f"ç¬¬ä¸‰é˜¶æ®µ: æ€§èƒ½å¯¹æ¯”åˆ†æ")
        print(f"="*60)
        
        analysis = {
            'summary': {},
            'recommendations': [],
            'performance_ratios': {},
            'device_comparison': {},
            'model_comparison': {}
        }
        
        # è·å–æµ‹è¯•é…ç½®
        devices_tested = results['test_config']['devices_tested']
        models_tested = results['test_config']['models_tested']
        single_results = results['single_image_results']
        
        # è®¾å¤‡æ€§èƒ½å¯¹æ¯”åˆ†æ
        print(f"\nğŸ“Š è®¾å¤‡æ€§èƒ½å¯¹æ¯”åˆ†æ:")
        print("-" * 50)
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ†æè®¾å¤‡æ€§èƒ½
        for model_key in models_tested:
            model_name = results['model_configs'][model_key]['name']
            print(f"\næ¨¡å‹: {model_name}")
            print(f"{'è®¾å¤‡':<8} {'å¹³å‡æ—¶é—´(ms)':<12} {'ååé‡(fps)':<12} {'å†…å­˜(MB)':<10}")
            print("-" * 45)
            
            model_device_performance = {}
            for device in devices_tested:
                if device in single_results.get(model_key, {}):
                    result = single_results[model_key][device]
                    if 'error' not in result:
                        avg_time = result['avg_time_ms']
                        throughput = 1000 / avg_time  # fps
                        memory = result['avg_memory_mb']
                        
                        model_device_performance[device] = {
                            'avg_time_ms': avg_time,
                            'throughput_fps': throughput,
                            'memory_mb': memory
                        }
                        
                        print(f"{device.upper():<8} {avg_time:<12.2f} {throughput:<12.2f} {memory:<10.2f}")
            
            # è®¡ç®—ç›¸å¯¹äºCPUçš„åŠ é€Ÿæ¯”
            if 'cpu' in model_device_performance and len(model_device_performance) > 1:
                cpu_throughput = model_device_performance['cpu']['throughput_fps']
                print(f"  è®¾å¤‡åŠ é€Ÿæ¯” (ç›¸å¯¹äºCPU):")
                
                for device, perf in model_device_performance.items():
                    if device != 'cpu':
                        speedup = perf['throughput_fps'] / cpu_throughput
                        analysis['performance_ratios'][f'{model_key}_{device}_vs_cpu'] = speedup
                        
                        status = "ğŸš€" if speedup > 2.0 else "ğŸ“ˆ" if speedup > 1.2 else "âš ï¸" if speedup > 0.8 else "ğŸŒ"
                        print(f"    {device.upper()}: {speedup:.2f}x {status}")
                        
                        # ç”Ÿæˆè®¾å¤‡å»ºè®®
                        if speedup > 2.0:
                            analysis['recommendations'].append(
                                f"{model_name} åœ¨ {device.upper()} ä¸Šæ˜¾è‘—ä¼˜äº CPU ({speedup:.1f}x)ï¼Œæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ"
                            )
                        elif speedup < 0.8:
                            analysis['recommendations'].append(
                                f"{model_name} åœ¨ {device.upper()} ä¸Šæ€§èƒ½ä¸å¦‚ CPUï¼Œå»ºè®®ä½¿ç”¨ CPU å¤„ç†"
                            )
            
            analysis['device_comparison'][model_key] = model_device_performance
        
        # æ¨¡å‹é—´æ€§èƒ½å¯¹æ¯”
        print(f"\nğŸ† æ¨¡å‹é—´æ€§èƒ½å¯¹æ¯” (åœ¨CPUä¸Š):")
        print("-" * 50)
        print(f"{'æ¨¡å‹':<20} {'å¹³å‡æ—¶é—´(ms)':<12} {'ååé‡(fps)':<12} {'å†…å­˜(MB)':<10}")
        print("-" * 55)
        
        cpu_model_performances = {}
        for model_key in models_tested:
            if 'cpu' in single_results.get(model_key, {}):
                result = single_results[model_key]['cpu']
                if 'error' not in result:
                    model_name = results['model_configs'][model_key]['name']
                    avg_time = result['avg_time_ms']
                    throughput = 1000 / avg_time
                    memory = result['avg_memory_mb']
                    
                    cpu_model_performances[model_key] = {
                        'name': model_name,
                        'avg_time_ms': avg_time,
                        'throughput_fps': throughput,
                        'memory_mb': memory
                    }
                    
                    print(f"{model_name:<20} {avg_time:<12.2f} {throughput:<12.2f} {memory:<10.2f}")
        
        # æ‰¾å‡ºæœ€å¿«çš„æ¨¡å‹ä½œä¸ºåŸºå‡†
        if cpu_model_performances:
            fastest_model = max(cpu_model_performances.items(), key=lambda x: x[1]['throughput_fps'])
            fastest_key, fastest_perf = fastest_model
            
            print(f"\nğŸ¥‡ æœ€å¿«æ¨¡å‹: {fastest_perf['name']} ({fastest_perf['throughput_fps']:.2f} fps)")
            print(f"æ¨¡å‹æ€§èƒ½æ’å:")
            
            # æŒ‰ååé‡æ’åº
            sorted_models = sorted(cpu_model_performances.items(), 
                                 key=lambda x: x[1]['throughput_fps'], reverse=True)
            
            for i, (model_key, perf) in enumerate(sorted_models, 1):
                ratio = perf['throughput_fps'] / fastest_perf['throughput_fps']
                print(f"  {i}. {perf['name']}: {perf['throughput_fps']:.2f} fps ({ratio:.2f}x)")
                analysis['model_comparison'][model_key] = {
                    'rank': i,
                    'relative_performance': ratio,
                    **perf
                }
        
        # æ‰¹å¤„ç†æ€§èƒ½åˆ†æ
        batch_results = results['batch_processing_results']
        print(f"\nğŸ“¦ æ‰¹å¤„ç†æ€§èƒ½æœ€ä¼˜é…ç½®:")
        print("-" * 50)
        
        for model_key in models_tested:
            model_name = results['model_configs'][model_key]['name']
            print(f"\n{model_name}:")
            
            for device in devices_tested:
                if device in batch_results.get(model_key, {}):
                    result = batch_results[model_key][device]
                    if 'error' not in result and 'batch_results' in result:
                        batch_data = result['batch_results']
                        if batch_data:
                            best_batch = max(batch_data.items(), key=lambda x: x[1]['avg_throughput'])
                            best_size, best_perf = best_batch
                            
                            print(f"  {device.upper()}: æ‰¹æ¬¡å¤§å° {best_size}, "
                                  f"ååé‡ {best_perf['avg_throughput']:.2f} å›¾åƒ/ç§’")
                            
                            analysis['summary'][f'{model_key}_{device}_best_batch_size'] = best_size
                            analysis['summary'][f'{model_key}_{device}_best_throughput'] = best_perf['avg_throughput']
        
        # ç”Ÿæˆæ€»ä½“å»ºè®®
        print(f"\nğŸ’¡ æ€»ä½“å»ºè®®:")
        print("-" * 40)
        
        if not analysis['recommendations']:
            analysis['recommendations'].append("æ‰€æœ‰æµ‹è¯•æ¨¡å‹åœ¨ä¸åŒè®¾å¤‡ä¸Šçš„æ€§èƒ½è¡¨ç°ç›¸è¿‘ï¼Œå¯æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©")
        
        # æ·»åŠ åŸºäºæ€§èƒ½çš„é€šç”¨å»ºè®®
        if cpu_model_performances:
            fastest_key = max(cpu_model_performances.items(), key=lambda x: x[1]['throughput_fps'])[0]
            fastest_name = cpu_model_performances[fastest_key]['name']
            analysis['recommendations'].append(f"æ¨èä½¿ç”¨ {fastest_name} ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
            
            # å†…å­˜ä½¿ç”¨å»ºè®®
            lowest_memory = min(cpu_model_performances.items(), key=lambda x: x[1]['memory_mb'])
            if lowest_memory[1]['memory_mb'] > 0:
                analysis['recommendations'].append(
                    f"å†…å­˜å—é™ç¯å¢ƒæ¨èä½¿ç”¨ {lowest_memory[1]['name']} (å†…å­˜ä½¿ç”¨: {lowest_memory[1]['memory_mb']:.1f} MB)"
                )
        
        for i, recommendation in enumerate(analysis['recommendations'], 1):
            print(f"  {i}. {recommendation}")
        
        return analysis
    
    def save_results(self, results: Dict, output_file: str = "vision_model_benchmark_results.json"):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def print_device_status(self):
        """æ‰“å°è®¾å¤‡çŠ¶æ€ä¿¡æ¯"""
        print(f"\nğŸ–¥ï¸ è®¾å¤‡çŠ¶æ€æŠ¥å‘Š:")
        print("-" * 40)
        
        for device in self.available_devices:
            info = self.device_info[device]
            print(f"  {device.upper()}: {info['name']}")
            
            if device == 'cpu':
                # CPU è¯¦ç»†ä¿¡æ¯
                cpu_freq = psutil.cpu_freq()
                if cpu_freq:
                    print(f"    é¢‘ç‡: {cpu_freq.current:.0f} MHz")
                print(f"    ä½¿ç”¨ç‡: {psutil.cpu_percent(interval=1):.1f}%")
                
            elif device == 'mps':
                # MPS çŠ¶æ€
                if hasattr(torch.mps, 'current_allocated_memory'):
                    mps_memory = torch.mps.current_allocated_memory() / (1024**2)
                    print(f"    å½“å‰å†…å­˜ä½¿ç”¨: {mps_memory:.1f} MB")
                print(f"    çŠ¶æ€: å¯ç”¨")
                
            elif device == 'cuda':
                # CUDA è¯¦ç»†ä¿¡æ¯
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                allocated = torch.cuda.memory_allocated() / (1024**3)
                print(f"    æ€»å†…å­˜: {gpu_memory:.1f} GB")
                print(f"    å·²åˆ†é…: {allocated:.1f} GB")
                print(f"    åˆ©ç”¨ç‡: {allocated/gpu_memory*100:.1f}%")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤šæ¨¡å‹è§†è§‰ AI æ€§èƒ½åŸºå‡†æµ‹è¯•')
    parser.add_argument('--test_dir', type=str, default='benchmark_test_images', 
                        help='æµ‹è¯•å›¾åƒç›®å½•')
    parser.add_argument('--num_images', type=int, default=50, 
                        help='æµ‹è¯•å›¾åƒæ•°é‡')
    parser.add_argument('--models', type=str, nargs='+', required=False,
                        help='è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨ (å¿…éœ€å‚æ•°) - å¯é€‰: convnext_v2_large, resnet50, open_clip_vit_g14, openai_clip_vit_l14_336')
    parser.add_argument('--devices', type=str, nargs='+',
                        help='è¦æµ‹è¯•çš„è®¾å¤‡åˆ—è¡¨ (cpu, mps, cuda)')
    parser.add_argument('--output', type=str, default='vision_model_benchmark_results.json',
                        help='ç»“æœè¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--single_only', action='store_true',
                        help='ä»…è¿è¡Œå•å¼ å›¾åƒæµ‹è¯•')
    parser.add_argument('--batch_only', action='store_true',
                        help='ä»…è¿è¡Œæ‰¹å¤„ç†æµ‹è¯•')
    parser.add_argument('--list_models', action='store_true',
                        help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹å¹¶é€€å‡º')
    
    args = parser.parse_args()
    
    print("å¤šæ¨¡å‹è§†è§‰ AI æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    
    # åˆå§‹åŒ–åŸºå‡†æµ‹è¯•
    benchmark = VisionModelBenchmark(
        test_image_dir=args.test_dir,
        num_test_images=args.num_images
    )
    
    # å¦‚æœåªæ˜¯åˆ—å‡ºæ¨¡å‹ï¼Œåˆ™æ˜¾ç¤ºå¹¶é€€å‡º
    if args.list_models:
        print("\nğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
        print("=" * 50)
        available_models = benchmark.get_available_models()
        
        for model_key in benchmark.model_configs:
            config = benchmark.model_configs[model_key]
            status = "âœ…" if model_key in available_models else "âŒ"
            print(f"{status} {model_key}:")
            print(f"   åç§°: {config['name']}")
            print(f"   æè¿°: {config['description']}")
            print(f"   è¾“å…¥å°ºå¯¸: {config['input_size']}")
            print(f"   ç‰¹å¾ç»´åº¦: {config['expected_feature_dim']}")
            if model_key not in available_models:
                if config['model_type'] == 'open_clip':
                    print(f"   âš ï¸ éœ€è¦å®‰è£…: pip install open_clip_torch")
                elif config['model_type'] == 'clip':
                    print(f"   âš ï¸ éœ€è¦å®‰è£…: pip install git+https://github.com/openai/CLIP.git")
            print()
        
        return
    
    # æ‰“å°è®¾å¤‡çŠ¶æ€
    benchmark.print_device_status()
    
    # ç¡®å®šè¦æµ‹è¯•çš„æ¨¡å‹
    if not args.models:
        print(f"âŒ è¯·æŒ‡å®šè¦æµ‹è¯•çš„æ¨¡å‹ï¼")
        print(f"ä½¿ç”¨ --models å‚æ•°æŒ‡å®šä¸€ä¸ªæˆ–å¤šä¸ªæ¨¡å‹")
        print(f"å¯ç”¨æ¨¡å‹: {', '.join(benchmark.get_available_models())}")
        print(f"\nç¤ºä¾‹:")
        print(f"  python {sys.argv[0]} --models convnext_v2_large")
        print(f"  python {sys.argv[0]} --models convnext_v2_large resnet50")
        print(f"  python {sys.argv[0]} --list_models  # æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹")
        sys.exit(1)
    
    test_models = [m for m in args.models if m in benchmark.get_available_models()]
    if not test_models:
        print(f"âŒ æŒ‡å®šçš„æ¨¡å‹éƒ½ä¸å¯ç”¨: {args.models}")
        print(f"å¯ç”¨æ¨¡å‹: {', '.join(benchmark.get_available_models())}")
        sys.exit(1)
    
    # ç¡®å®šè¦æµ‹è¯•çš„è®¾å¤‡
    if args.devices:
        test_devices = [d for d in args.devices if d in benchmark.available_devices]
        if not test_devices:
            print(f"âŒ æŒ‡å®šçš„è®¾å¤‡éƒ½ä¸å¯ç”¨: {args.devices}")
            print(f"å¯ç”¨è®¾å¤‡: {', '.join(benchmark.available_devices)}")
            sys.exit(1)
    else:
        test_devices = benchmark.available_devices
    
    print(f"\nğŸ¯ å°†æµ‹è¯•ä»¥ä¸‹æ¨¡å‹: {', '.join([benchmark.model_configs[m]['name'] for m in test_models])}")
    print(f"ğŸ–¥ï¸ å°†æµ‹è¯•ä»¥ä¸‹è®¾å¤‡: {', '.join(d.upper() for d in test_devices)}")
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    if args.single_only:
        # ä»…å•å¼ å›¾åƒæµ‹è¯•
        print(f"\nğŸ” è¿è¡Œå•å¼ å›¾åƒæµ‹è¯•...")
        results = {
            'model_configs': {k: benchmark.model_configs[k] for k in test_models},
            'device_info': benchmark.device_info,
            'single_image_results': {}
        }
        
        for model_key in test_models:
            results['single_image_results'][model_key] = {}
            for device in test_devices:
                try:
                    result = benchmark.benchmark_single_image(model_key, device, num_runs=20)
                    results['single_image_results'][model_key][device] = result
                except Exception as e:
                    print(f"âŒ {model_key.upper()} on {device.upper()} æµ‹è¯•å¤±è´¥: {e}")
                    results['single_image_results'][model_key][device] = {'error': str(e)}
                
    elif args.batch_only:
        # ä»…æ‰¹å¤„ç†æµ‹è¯•
        print(f"\nğŸ“¦ è¿è¡Œæ‰¹å¤„ç†æµ‹è¯•...")
        results = {
            'model_configs': {k: benchmark.model_configs[k] for k in test_models},
            'device_info': benchmark.device_info,
            'batch_processing_results': {}
        }
        
        for model_key in test_models:
            results['batch_processing_results'][model_key] = {}
            for device in test_devices:
                try:
                    result = benchmark.benchmark_batch_processing(model_key, device)
                    results['batch_processing_results'][model_key][device] = result
                except Exception as e:
                    print(f"âŒ {model_key.upper()} on {device.upper()} æµ‹è¯•å¤±è´¥: {e}")
                    results['batch_processing_results'][model_key][device] = {'error': str(e)}
    else:
        # ç»¼åˆæµ‹è¯•
        results = benchmark.run_comprehensive_benchmark(test_models, test_devices)
    
    # ä¿å­˜ç»“æœ
    benchmark.save_results(results, args.output)
    
    print(f"\nğŸ‰ åŸºå‡†æµ‹è¯•å®Œæˆ!")
    print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {args.output}")


if __name__ == "__main__":
    main() 