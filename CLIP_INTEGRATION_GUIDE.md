# CLIP Models Integration Guide

## Overview

æœ¬é¡¹ç›®å·²æˆåŠŸé›†æˆäº†ä¸¤ä¸ªä¸šç•Œé¡¶çº§çš„CLIPï¼ˆContrastive Language-Image Pre-trainingï¼‰æ¨¡å‹å®ç°ï¼š

- **OpenAI CLIP**: å®˜æ–¹åŸç”Ÿå®ç°ï¼Œä½œä¸ºåŸºå‡†
- **OpenCLIP**: åœ¨æ›´å¤§æ•°æ®é›†ä¸Šè®­ç»ƒçš„å¼€æºå¢å¼ºç‰ˆæœ¬ï¼Œæ€§èƒ½æ›´ä¼˜

è¿™äº›æ¨¡å‹æä¾›äº†å“è¶Šçš„å›¾åƒè¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œé€šè¿‡æ–‡æœ¬-å›¾åƒå¯¹æ¯”å­¦ä¹ è®­ç»ƒè€Œæˆã€‚

## åŠŸèƒ½ç‰¹æ€§

### âœ¨ æ ¸å¿ƒä¼˜åŠ¿

1. **è¯­ä¹‰ç†è§£å¢å¼º**: CLIPæ¨¡å‹é€šè¿‡æ–‡æœ¬-å›¾åƒå¯¹æ¯”å­¦ä¹ ï¼Œæä¾›æ›´å¥½çš„è¯­ä¹‰ç‰¹å¾è¡¨ç¤º
2. **æ€§èƒ½å“è¶Š**: ç‰¹åˆ«æ˜¯OpenCLIPåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„è®­ç»ƒç‰ˆæœ¬
3. **å®Œå…¨å…¼å®¹**: ä¸ç°æœ‰torchvisionæ¨¡å‹æ— ç¼é›†æˆ
4. **æ¨¡å‹å¤šæ ·æ€§**: æ”¯æŒå¤šç§æ¶æ„ï¼ˆViTã€ResNetã€ConvNeXtï¼‰
5. **çµæ´»é…ç½®**: æ”¯æŒæ··åˆæ¨¡å‹é›†æˆå’Œå•ç‹¬ä½¿ç”¨

### ğŸ”§ æŠ€æœ¯å®ç°

- **æ™ºèƒ½æ¨¡å‹åŠ è½½**: è‡ªåŠ¨è¯†åˆ«æ¨¡å‹ç±»å‹å¹¶ä½¿ç”¨å¯¹åº”çš„åŠ è½½æ–¹æ³•
- **é¢„å¤„ç†è‡ªé€‚åº”**: CLIPæ¨¡å‹ä½¿ç”¨ä¸“é—¨çš„é¢„å¤„ç†ç®¡é“
- **ç‰¹å¾æå–ä¼˜åŒ–**: ä½¿ç”¨`encode_image()`æ–¹æ³•è·å¾—æœ€ä½³ç‰¹å¾è¡¨ç¤º
- **æ‰¹é‡å¤„ç†æ”¯æŒ**: æ”¯æŒå•å¼ å›¾ç‰‡å’Œæ‰¹é‡ç›®å½•å¤„ç†
- **è®¾å¤‡è‡ªé€‚åº”**: æ”¯æŒCPUå’ŒGPUæ¨ç†

## æ”¯æŒçš„æ¨¡å‹

### OpenAI CLIP æ¨¡å‹

| æ¨¡å‹åç§° | ç‰¹å¾ç»´åº¦ | è¾“å…¥å°ºå¯¸ | æè¿° |
|---------|---------|---------|------|
| `openai_clip_vit_b32` | 512 | 224x224 | ViT-B/32 åŸºç¡€ç‰ˆæœ¬ |
| `openai_clip_vit_b16` | 512 | 224x224 | ViT-B/16 é«˜ç²¾åº¦ç‰ˆæœ¬ |
| `openai_clip_vit_l14` | 768 | 224x224 | ViT-L/14 å¤§å‹ç‰ˆæœ¬ |
| `openai_clip_rn50` | 1024 | 224x224 | ResNet-50 ç‰ˆæœ¬ |
| `openai_clip_rn101` | 512 | 224x224 | ResNet-101 ç‰ˆæœ¬ |

### OpenCLIP æ¨¡å‹ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰

| æ¨¡å‹åç§° | ç‰¹å¾ç»´åº¦ | è¾“å…¥å°ºå¯¸ | æè¿° |
|---------|---------|---------|------|
| `open_clip_vit_b32_openai` | 512 | 224x224 | ViT-B/32 OpenAIæ•°æ®é›† |
| `open_clip_vit_b32_laion400m` | 512 | 224x224 | ViT-B/32 LAION-400M |
| `open_clip_vit_b16_laion400m` | 512 | 224x224 | ViT-B/16 LAION-400M |
| `open_clip_vit_l14_laion2b` | 768 | 224x224 | ViT-L/14 LAION-2B |
| `open_clip_vit_h14_laion2b` | 1024 | 224x224 | ViT-H/14 LAION-2B |
| `open_clip_convnext_base` | 512 | 256x256 | ConvNeXt-Base |
| `open_clip_convnext_large` | 768 | 320x320 | ConvNeXt-Large |

## å®‰è£…ä¾èµ–

```bash
# æ–¹æ³•1: ä½¿ç”¨ pip å®‰è£…
pip install git+https://github.com/openai/CLIP.git
pip install open-clip-torch

# æ–¹æ³•2: æˆ–è€…ä»requirements.txtå®‰è£…
pip install -r requirements.txt
```

## ä½¿ç”¨ç¤ºä¾‹

### 1. ä½¿ç”¨å•ä¸ªCLIPæ¨¡å‹

```bash
# OpenAI CLIP ViT-B/32
python examples/image_vectorizer.py --image_path image.jpg --models openai_clip_vit_b32

# OpenCLIP å¢å¼ºç‰ˆæœ¬
python examples/image_vectorizer.py --image_path image.jpg --models open_clip_vit_h14_laion2b
```

### 2. æ‰¹é‡å¤„ç†

```bash
# ä½¿ç”¨CLIPæ¨¡å‹æ‰¹é‡å¤„ç†ç›®å½•
python examples/image_vectorizer.py --image_path ./images/ --models openai_clip_vit_l14 --batch_size 8
```

### 3. æ··åˆæ¨¡å‹é›†æˆ

```bash
# CLIP + ä¼ ç»Ÿæ¨¡å‹é›†æˆ
python examples/image_vectorizer.py --image_path image.jpg --models openai_clip_vit_l14,resnet152

# å¤šCLIPæ¨¡å‹é›†æˆ
python examples/image_vectorizer.py --image_path image.jpg --models openai_clip_vit_b32,open_clip_vit_h14_laion2b
```

### 4. GPU åŠ é€Ÿ

```bash
# ä½¿ç”¨GPUè¿›è¡ŒCLIPæ¨ç†
python examples/image_vectorizer.py --image_path image.jpg --models open_clip_vit_h14_laion2b --device gpu
```

## è¾“å‡ºç¤ºä¾‹

```
ğŸš€ å›¾åƒå‘é‡åŒ–å·¥å…·å¯åŠ¨
==================================================
é€‰æ‹©çš„æ¨¡å‹: ['openai_clip_vit_l14']
æ­£åœ¨è®¾ç½®æ¨¡å‹é›†æˆ ['openai_clip_vit_l14'] åœ¨è®¾å¤‡: 'cpu'
æ­£åœ¨åŠ è½½å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹...
==================================================
æ­£åœ¨åŠ è½½: OpenAI CLIP ViT-L/14 (768ç»´) (openai_clip_vit_l14)
  æ­£åœ¨ä¸‹è½½ OpenAI CLIP æ¨¡å‹: ViT-L/14...
  openai_clip_vit_l14 å·²åŠ è½½åˆ°è®¾å¤‡: cpu
âœ“ openai_clip_vit_l14 åŠ è½½å®Œæˆ (ç»´åº¦: 768, è€—æ—¶: 0.02ç§’)
==================================================
âœ“ æ¨¡å‹é›†æˆåˆå§‹åŒ–å®Œæˆï¼
  åŠ è½½æˆåŠŸçš„æ¨¡å‹: ['openai_clip_vit_l14']
  æ€»ç‰¹å¾ç»´åº¦: 768
  è®¡ç®—å¤æ‚åº¦å€æ•°: 1x
==================================================

ğŸ“± è®¾å¤‡ä¿¡æ¯:
  è®¾å¤‡: CPU

ğŸ–¼ï¸ å•å›¾åƒå¤„ç†æ¨¡å¼
å›¾åƒè·¯å¾„: test_images/test_image_001.jpg

==================================================
ğŸ‰ å‘é‡åŒ–å®Œæˆ!
------------------------------
è®¾å¤‡: cpu
æ¨¡å‹: ['openai_clip_vit_l14'] (1ä¸ªæ¨¡å‹)
å¤„ç†æ—¶é—´: 0.8521 ç§’
ç‰¹å¾å‘é‡å½¢çŠ¶: (768,)
å‰10ä¸ªå…ƒç´ : [ 0.12345  -0.67891   0.23456  -0.78912   0.34567  ...]
ç»Ÿè®¡ä¿¡æ¯: å‡å€¼=0.0123, æ ‡å‡†å·®=0.4567
```

## æ€§èƒ½å¯¹æ¯”

### ç‰¹å¾ç»´åº¦å¯¹æ¯”

| æ¨¡å‹ç±»å‹ | æ¨¡å‹ | ç‰¹å¾ç»´åº¦ | è¯­ä¹‰ç†è§£ | è®¡ç®—å¤æ‚åº¦ |
|---------|------|---------|---------|-----------|
| ä¼ ç»Ÿ | ResNet-152 | 2048 | â­â­â­ | â­â­â­ |
| ä¼ ç»Ÿ | EfficientNet-B7 | 2560 | â­â­â­ | â­â­â­â­ |
| CLIP | OpenAI ViT-B/32 | 512 | â­â­â­â­â­ | â­â­ |
| CLIP | OpenAI ViT-L/14 | 768 | â­â­â­â­â­ | â­â­â­ |
| CLIP | OpenCLIP ViT-H/14 | 1024 | â­â­â­â­â­ | â­â­â­â­ |

### æ¨èä½¿ç”¨åœºæ™¯

1. **å¿«é€ŸåŸå‹**: `openai_clip_vit_b32` - å¹³è¡¡çš„æ€§èƒ½å’Œé€Ÿåº¦
2. **é«˜è´¨é‡åº”ç”¨**: `open_clip_vit_h14_laion2b` - æœ€ä½³è¯­ä¹‰ç†è§£
3. **èµ„æºå—é™**: `openai_clip_vit_b32` - æœ€å°çš„è®¡ç®—å¼€é”€
4. **æ‰¹é‡å¤„ç†**: `open_clip_vit_l14_laion2b` - è‰¯å¥½çš„æ‰¹å¤„ç†æ€§èƒ½

## æŠ€æœ¯æ¶æ„

### æ¨¡å‹åŠ è½½æµç¨‹

```python
# è‡ªåŠ¨è¯†åˆ«æ¨¡å‹ç±»å‹
model_type = model_config.get('model_type', 'torchvision')

if model_type == 'openai_clip':
    # OpenAI CLIPåŠ è½½
    model, preprocessor = clip.load(model_name, device=device)
elif model_type == 'open_clip':
    # OpenCLIPåŠ è½½
    model, _, preprocessor = open_clip.create_model_and_transforms(
        model_name, pretrained=pretrained, device=device
    )
```

### ç‰¹å¾æå–æµç¨‹

```python
# æ ¹æ®æ¨¡å‹ç±»å‹ä½¿ç”¨ä¸åŒçš„æ¨ç†æ–¹æ³•
if model_type == 'torchvision':
    features = feature_extractor(image_tensor)
elif model_type in ['openai_clip', 'open_clip']:
    features = feature_extractor.encode_image(image_tensor)
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **ä¾èµ–å®‰è£…å¤±è´¥**
   ```bash
   # è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨å®˜æ–¹GitHubä»“åº“
   pip install git+https://github.com/openai/CLIP.git
   ```

2. **å†…å­˜ä¸è¶³**
   ```bash
   # è§£å†³æ–¹æ¡ˆï¼šå‡å°æ‰¹å¤„ç†å¤§å°
   --batch_size 2
   ```

3. **æ¨¡å‹ä¸‹è½½ç¼“æ…¢**
   ```bash
   # è§£å†³æ–¹æ¡ˆï¼šæ¨¡å‹ä¼šç¼“å­˜åˆ°~/.cache/ï¼Œé¦–æ¬¡ä¸‹è½½è¾ƒæ…¢
   ```

4. **GPUå†…å­˜ä¸è¶³**
   ```bash
   # è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨CPUæˆ–è¾ƒå°çš„æ¨¡å‹
   --device cpu --models openai_clip_vit_b32
   ```

## å‘åå…¼å®¹æ€§

âœ… **å®Œå…¨å…¼å®¹**: æ‰€æœ‰ç°æœ‰çš„torchvisionæ¨¡å‹ï¼ˆResNetã€EfficientNetã€ViTï¼‰ç»§ç»­æ­£å¸¸å·¥ä½œ

âœ… **ç°æœ‰è„šæœ¬æ— éœ€ä¿®æ”¹**: åŸæœ‰çš„å‘½ä»¤è¡Œå‚æ•°å’Œç”¨æ³•ä¿æŒä¸å˜

âœ… **æ¸è¿›å¼è¿ç§»**: å¯ä»¥é€æ­¥å°†CLIPæ¨¡å‹åŠ å…¥ç°æœ‰çš„æ¨¡å‹é›†æˆä¸­

## æœªæ¥æ‰©å±•

- [ ] æ”¯æŒæ›´å¤šOpenCLIPé¢„è®­ç»ƒæƒé‡
- [ ] é›†æˆå¤šæ¨¡æ€æ–‡æœ¬-å›¾åƒæ£€ç´¢åŠŸèƒ½
- [ ] æ·»åŠ æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] æ”¯æŒè‡ªå®šä¹‰CLIPå¾®è°ƒæ¨¡å‹

## è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›CLIPé›†æˆï¼

---

**æ³¨æ„**: é¦–æ¬¡ä½¿ç”¨CLIPæ¨¡å‹æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ä¸‹è½½é¢„è®­ç»ƒæƒé‡ï¼Œè¯·ç¡®ä¿ç½‘ç»œè¿æ¥ç¨³å®šã€‚æ¨¡å‹æƒé‡ä¼šç¼“å­˜åˆ°æœ¬åœ°ï¼Œåç»­ä½¿ç”¨å°†ä¼šæ›´å¿«ã€‚